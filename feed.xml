<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.4">Jekyll</generator><link href="https://rectifiedflow.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://rectifiedflow.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2024-12-23T16:12:18+00:00</updated><id>https://rectifiedflow.github.io/feed.xml</id><title type="html">blank</title><subtitle>Everything should be made as simple as possible, but not simpler. </subtitle><entry><title type="html">DDIM and Natural Euler Samplers</title><link href="https://rectifiedflow.github.io/blog/2024/discretization/" rel="alternate" type="text/html" title="DDIM and Natural Euler Samplers"/><published>2024-12-10T11:00:00+00:00</published><updated>2024-12-10T11:00:00+00:00</updated><id>https://rectifiedflow.github.io/blog/2024/discretization</id><content type="html" xml:base="https://rectifiedflow.github.io/blog/2024/discretization/"><![CDATA[ <h2 id="overview">Overview</h2> <p>Rectified flow learns an ODE of the form \(\mathrm{d}Z_t = v_t(Z_t; \theta)\) by minimizing the loss between its velocity field and that of an interpolation process. As discussed in a <a href="../interpolation/#affine-interpolations-are-pointwise-transformable">previous blog</a>, different affine interpolations yield equivalent velocity fields and identical noise-data couplings.</p> <p>In practice, we must approximate ODEs with <strong>discrete solvers</strong>. A standard approach is to use the Euler method: at each step, we approximate the local trajectory by a tangent line. For rectified flows induced by straight interpolations, this approach is intuitive. However, if the interpolation is nonlinear (e.g., spherical), a more natural choice is to approximate each step with a curved segment that matches the interpolation. We refer to these methods as <strong>natural Euler samplers</strong>.</p> <div class="l-body"> <figure id="figure-svg"> <object data="/assets/img/natural_euler.svg" type="image/svg+xml" width="100%" height="200px"> <p> Your browser does not support embedded SVG. <a href="/assets/img/natural_euler.svg">Download here</a>. </p> </object> <figcaption> <a href="#figure-natural-euler"></a> </figcaption> </figure> </div> <p>Notably, the DDIM sampler can be interpreted as a natural Euler sampler under its corresponding interpolation. From this perspective, we do not need complicated coefficient derivations for DDIM’s inference steps.</p> <p>A key property of natural Euler samplers is:</p> <blockquote class="definition"> <p>If two interpolation processes are related by a pointwise transform, then <strong>their discrete trajectories obtained through natural Euler sampling are also related by the same pointwise transform</strong>, provided the time grids are appropriately scaled.</p> </blockquote> <p>In other words, <strong>when using natural Euler samplers, switching the affine interpolation scheme <em>at inference time</em> is essentially adjusting the sampling time grid.</strong> For DDIM, we obtain <strong>exactly the same discrete samples</strong> as those produced by a standard Euler solver applied to the rectified flow induced by straight interpolation, once we properly rescale the time grid.</p> <p>For a more comprehensive and rigorous discussion on this topic, please refer to Chapter 5 in the <a href="https://github.com/lqiang67/rectified-flow/tree/main/pdf">Rectified Flow Lecture Notes</a>.</p> <h2 id="affine-interpolation-solver">Affine Interpolation Solver</h2> <p>Consider a coupling \((X_0, X_1)\) of source distirbution \(X_0 \sim \pi_0\) and target unknown data distribution \(X_1 \sim \pi_1\). Define an interpolation function: \(\mathtt I:[0,1] \times \mathbb R^d \times \mathbb R^d \mapsto \mathbb R^d\) such that</p> \[\mathtt I_0(X_0, X_1) = X_0, \quad \mathtt I_1(X_0, X_1)=X_1.\] <p>We assume \(\mathtt I_t\) is differentiable in \(t\), and denote</p> \[X_t = \mathtt I_t(X_0, X_1), \quad \dot{X}_t = \partial_t \mathtt{I}_t(X_0, X_1).\] <p>In many cases, given sample and velocity \((X_t, \dot{X}_t)\), we want to recover \((X_0, X_1)\) that satisfy</p> \[\begin{cases} X_t = \mathtt{I}_t(X_0, X_1), \\[6pt] \dot{X}_t = \partial_t \mathtt{I}_t(X_0, X_1). \end{cases}\] <p>For affine interpolations \(X_t = \alpha_t X_1 + \beta_t X_0\), this problem reduces to solving a simple \(2 \times 2\) linear system. Moreover, due to the linearity of expectations, the following conditional expectations also inherit this linear structure.</p> <blockquote class="example"> <p><strong>Affine Interpolation Solvers.</strong></p> <p>Define</p> \[\begin{aligned} v_t(x) &amp;:= \mathbb{E}[\dot{X}_t \mid X_t = x], &amp;\text{(RF velocity field)} \\[6pt] \hat{x}_{0\mid t}(x) &amp;:= \mathbb{E}[X_0 \mid X_t = x], &amp;\text{(Expected noise $X_0$)} \\[6pt] \hat{x}_{1\mid t}(x) &amp;:= \mathbb{E}[X_1 \mid X_t = x]. &amp;\text{(Expected data $X_1$)} \end{aligned}\] <p>Given any two of \(\{\dot X_t, X_0, X_1, X_t\}\) (from interpolation) or \(\{v_t, \hat x_{0\mid t}, \hat x_{1\mid t}, x_t\}\) (from rectified flow), we can solve explicitly for the other two that satisfy</p> \[X_t = \alpha_t X_1 + \beta_t X_0, \quad \dot{X}_t = \dot{\alpha}_t X_1 + \dot{\beta}_t X_0. \\\] <p>or</p> \[x_t =\alpha_t\hat{x}_{1\mid t} + \beta_t \hat{x}_{0\mid t}, \quad v_t = \dot \alpha_t \hat{x}_{1\mid t} + \dot \beta_t \hat{x}_{0\mid t}\] <p>This can be efficiently implemented as <a href="https://github.com/lqiang67/rectified-flow/blob/main/rectified_flow/flow_components/interpolation_solver.py">affine interpolation solvers</a>.</p> </blockquote> <h2 id="natural-euler-sampler">Natural Euler Sampler</h2> <p>The standard Euler method approximates the flow \(\{Z_t\}\) on a discrete time grid \(\{t_i\}\) by using locally linear steps:</p> \[\hat{z}_{t_{i+1}} = \hat{z}_{t_i} + (t_{i+1} - t_i) \cdot v_{t_i}(\hat{z}_{t_i}).\] <p>This treats the local trajectory as a straight line tangent at \(\hat{z}_{t_i}\).</p> <p>In contrast, the <strong>natural Euler sampler</strong> uses the interpolation curve that is tangent at \(\hat{z}_{t_i}\). Instead of advancing along a straight line, it follows a locally curved trajectory consistent with the given interpolation scheme. The update rule is:</p> \[\hat{z}_{t_{i+1}} = \mathtt{I}_{t_{i+1}}(\hat{x}_{0 \mid t_i}, \hat{x}_{1 \mid t_i}),\] <p>where \(\hat{x}_{0 \mid t_i}\) and \(\hat{x}_{1 \mid t_i}\) are determined by identifying the interpolation curve that passes through \(\hat{z}_{t_i}\) and satisfies \(\partial \mathtt{I}_{t_i}(\hat{x}_{0 \mid t_i}, \hat{x}_{1 \mid t_i}) = v_{t_i}(\hat{z}_{t_i}).\) In other words, we first find the specific interpolation curve \(\mathtt{I}\) that matches the given slope at \(\hat{z}_{t_i}\), then advance one step along it.</p> <p>For instance, the natural euler sampler under spherical RF is</p> <blockquote class="example"> <p><strong>Example 1. Natural Euler Sampler for Spherical Interpolation</strong></p> \[\hat z_{t + \epsilon} =\cos\left(\frac{\pi}{2} \cdot\epsilon\right) \cdot \hat z_{t} + \frac{2}{\pi} \sin \left(\frac{\pi}{2} \cdot\epsilon\right) \cdot v_t(\hat z_t)\] </blockquote> <p>Another example is DDIM. Although it may appear complex, DDIM can be interpreted as a natural Euler sampler derived from a spherical interpolation.</p> <blockquote class="example"> <p><strong>Example 2. Natural Euler Sampler for DDIM</strong></p> <p>The discretized inference scheme of DDIM is a instance of natural Euler sampler for spherical interpolations satisfying \(\alpha_t^2 + \beta_t^2 = 1\). Note that the inference update of DDIM is written in terms of the expected noise \(\hat{x}_{0\mid t}(x) = \mathbb{E}[X_0 \mid X_t = x]\), we rewrite the update step using \(\hat{x}_{0 \mid t}\):</p> \[\begin{aligned} \hat{z}_{t+\epsilon} &amp;= \alpha_{t+\epsilon} \cdot\hat{x}_{1\vert t}(\hat{z}_t) + \beta_{t+\epsilon} \cdot \hat{x}_{0\vert t}(\hat{z}_t) \\ &amp;\overset{*}{=} \alpha_{t+\epsilon} \left( \frac{\hat{z}_t - \beta_t \cdot\hat{x}_{0\vert t}(\hat{z}_t)}{\alpha_t} \right) + \beta_{t+\epsilon} \cdot \hat{x}_{0\vert t}(\hat{z}_t) \\ &amp;= \frac{\alpha_{t+\epsilon}}{\alpha_t} \hat{z}_t + \left( \beta_{t+\epsilon} - \frac{\alpha_{t+\epsilon} \beta_t}{\alpha_t} \right) \hat{x}_{0\vert t}(\hat{z}_t) \end{aligned}\] <p>where in \(\overset{*}{=}\) we used \(\alpha_t \cdot \hat{x}_{1\vert t}(\hat{z}_t) + \beta_t\cdot \hat{x}_{0\vert t}(\hat{z}_t) = \hat{z}_t\). We can slightly rewrite the update as:</p> \[\frac{\hat{z}_{t+\epsilon}}{\alpha_{t+\epsilon}} = \frac{\hat{z}_t}{\alpha_t} + \left( \frac{\beta_{t+\epsilon}}{\alpha_{t+\epsilon}} - \frac{\beta_t}{\alpha_t} \right) \hat{x}_{0\vert t}(\hat{z}_t),\] <p>which matches Equation 13 of <d-cite key="song2020denoising"></d-cite>.</p> </blockquote> <h2 id="equivalence-of-natural-euler-trajectories">Equivalence of Natural Euler Trajectories</h2> <p>Natural Euler sampling preserves pointwise equivalences of discrete trajectory points across different interpolations:</p> <blockquote class="theorem"> <p><strong>Theorem 1. Equivalence of Natural Euler Trajectories</strong></p> <p>Suppose \(\{X_t\}\) and \(\{X_t'\}\) are two interpolation processes contructed from the same couping, related by a pointwise transform \(X_t' = \phi_t(X_{\tau_t})\). Consider the discrete trajectories \(\{\hat{z}_{t_i}\}_i\) and \(\{\hat{z}_{t_i'}'\}_i\), produced by the natural Euler samplers of rectified flows induced by \(\{X_t\}\) and \(\{X_t'\}\), on time grids \(\{t_i\}\) and \(\{t_i'\}\) respectively.</p> <p>If the time grids satisfy \(\tau(t_i') = t_i\) for all \(i\), and the initial conditions align via \(\hat{z}_{t_0}' = \phi(\hat{z}_{t_0})\), then the <strong>discrete</strong> trajectories also match under the same transform:</p> \[\hat{z}_{t_i'}' = \phi_{t_i}(\hat{z}_{t_i}) \quad \text{for all } i = 0,1,\ldots\] </blockquote> <p>Let \(\{X_t'\} = \texttt{Transform}(\{X_t\})\) denote a pointwise transformation, and let \(\texttt{NaturalEulerRF}\) represent the operation of generating discrete trajectories from a rectified flow ODE using the natural Euler sampler. Then:</p> \[\texttt{NaturalEulerRF}(\texttt{Transform}(\{X_t\})) = \texttt{Transform}(\texttt{NaturalEulerRF}(\{X_t\})).\] <p>As for the DDIM sampler, once we rescale the time grids appropriately, its discrete trajectories become equivalent to those generated by the standard Euler method on the corresponding straight RF.</p> <blockquote class="example"> <p><strong>Example 3. Equivalence of Straight Euler and DDIM sampler</strong></p> <p>Consider the natural Euler sampler applied to a RF induced by an affine interpolation \(\{X_t' = \alpha'_t X_1 + \beta'_t X_0\}\) using a uniform time grid \(t'_i = i/n\). This procedure is equivalent to applying the standard (straight) Euler method to the straight RF (induced by \(\{X_t = tX_1 + (1 - t)X_0\}\)), but with a non-uniform time grid:</p> \[t_i = \frac{\alpha'_{i/n}}{\alpha'_{i/n}+\beta'_{i/n}}.\] <p>Conversely, starting from the straight RF and applying the standard Euler sampler on \(\{t_i\}\), one can recover the natural Euler sampler for the affine interpolation by selecting a time grid \(\{t'_i\}\) such that:</p> \[t_i = \frac{\alpha'_{t'_{i}}}{\alpha'_{t'_{i}}+\beta'_{t'_{i}}}.\] <p>Since DDIM is the natural Euler sampler under spherical interpolation, and the straight-interpolation counterpart corresponds to the standard Euler method, appropriately scaling the time grid for the straight RF reproduces the results of the DDIM sampler exactly.</p> </blockquote> <p>Below, we compare two equivalent RFs: one induced by straight interpolation and the other induced by spherical interpolation. To ensure exact equivalence, we train a RF using the straight interpolation, then transform it into the spherical form.</p> <div class="l-body"> <figure id="figure-3"> <div style="display: flex;"> <iframe src="/assets/plotly/discrete_vanilla_euler_4_step.html" frameborder="0" scrolling="no" height="420px" width="45%"></iframe> <iframe src="/assets/plotly/discrete_natural_euler_4_step.html" frameborder="0" scrolling="no" height="420px" width="45%"></iframe> </div> <figcaption> <a href="#figure-1">Figure 1</a>. Comparison of the Vanilla Euler sampler (left) and the Natural Euler sampler (right) on spherical RF. The left approximates each step using straight segments, while the right uses local curves for each step. Zoom in to observe the differences in detail. </figcaption> </figure> </div> <div class="l-body"> <figure id="figure-2"> <iframe src="/assets/plotly/discrete_natural_double_match.html" frameborder="0" scrolling="no" height="430px" width="100%"> </iframe> <figcaption> <a href="#figure-2">Figure 2</a>. Sampling with the Natural Euler sampler on both Straight and Spherical RF. By adjusting the time grid of the Straight RF using \(\tau_t\) (while maintaining a uniform grid for the Spherical RF), the final generated results are identical. </figcaption> </figure> </div>]]></content><author><name>Runlong Liao</name></author><category term="tutorial"/><summary type="html"><![CDATA[Even discretized trajectories are equivalent]]></summary></entry><entry><title type="html">Interpolations: All Flows are One Flow</title><link href="https://rectifiedflow.github.io/blog/2024/interpolation/" rel="alternate" type="text/html" title="Interpolations: All Flows are One Flow"/><published>2024-12-10T10:00:00+00:00</published><updated>2024-12-10T10:00:00+00:00</updated><id>https://rectifiedflow.github.io/blog/2024/interpolation</id><content type="html" xml:base="https://rectifiedflow.github.io/blog/2024/interpolation/"><![CDATA[ <p>Most diffusion and flow models can be analyzed through the rectified flow lens, but they employ different interpolation methods, typically affine interpolations such as straight-line or spherical interpolations. A critical question is to understand the impact of using different interpolation processes. This blog introduces the equivalent relationships between rectified flows induced by these different interpolation processes, as discussed in Chapter 3 of these <a href="https://github.com/lqiang67/rectified-flow/tree/main/pdf">lecture notes</a>. Related observations and discussions can also be found in <d-cite key="karras2022elucidating,kingma2024understanding,shaulbespoke,gao2025diffusionmeetsflow"></d-cite>.</p> <h2 id="overview">Overview</h2> <p>Given a coupling \((X_0, X_1)\) of source distribution \(X_0\sim \pi_0\) and target unknown data distribution \(X_1 \sim \pi_1\), recall that rectified flow learns an ODE</p> \[\mathrm d Z_t = v_t(Z_t) \mathrm d t,\] <p>which, starts from the noise \(Z_0=X_0\), ends at generated data \(Z_1\). This velocity field \(v_t\) is learned by minimizing the mean square error from the slope of an interpolation process:</p> \[\min_v \int _0 ^1 \mathbb E \left[\left\| \dot X_t - v_t(X_t)\right\|^2 \right] \mathrm d t,\] <p>where \(\{X_t\} = \{X_t: t\in [0,1]\}\) is the interpolation process connecting \(X_0\) and \(X_1\), and \(\dot X_t\) denotes its time derivative. We call the ODE process \(\mathrm d Z_t = v_t(Z_t) \mathrm d t\) with \(Z_0=X_0\) the rectified flow induced from \(\{X_t\}\), and denote it as:</p> \[\{Z_t\} = \texttt {Rectify}(\{X_t\}),~~~~~~~~(Z_0,Z_1)= \texttt{RectifiedCoupling}(\{X_t\}),\] <p>where the noise-data pair \((Z_0,Z_1)\) generated by RF is called the rectified coupling of \(\{X_t\}\).</p> <p>In principle, any time-differentiable interpolation process, \(\{X_t\}\), connecting \(X_0\) and \(X_1\) can be used within this framework. Different existing methods employ various interpolation schemes. The simplest choice is straight interpolation, defined as \(X_t = t X_1 + (1-t) X_0\), which is also well-justified by optimal transport theory. However, other methods, such as DDIM and DDPM, use curved interpolation schemes of a more general affine form, \(X_t = \alpha_t X_1 + \beta_t X_0\), where \(\alpha_t\) and \(\beta_t\) are chosen differently.</p> <p>What is the impact of the choice of interpolation? Do different interpolation schemes yield fundamentally different rectified flow dynamics? At first glance, one might suspect that these choices, by influencing the learned rectified flow, could significantly affect inference performance and speed. This would imply that the interpolation scheme must be determined during training. It turns out this is not necessarily the case.</p> <p>In this blog, we demonstrate that if two interpolation processes are <em>pointwise transformable</em> (or <em>topologically equivalent</em>) in an appropriate sense, their rectified flows can also be pointwise transformed using the same maps, resulting in identical rectified couplings. Notably, any two affine interpolations are pointwise transformable, which indicates that it is essentially sufficient to focus on the simplest straight-line interpolation. Specifically, the use of different affine interpolations, and all other interpolations that are topologically equivalent to straight-line interpolations, can be addressed by employing straight-line interpolations, but with adjusted training losses and discretization algorithms. In particular, we show that using different affine interpolations is mathematically equivalent to incorporating time-weighting into the training loss. Further implications for discretization algorithms are discussed in this <a href="https://rectifiedflow.github.io/blog/2024/discretization/">blog</a>.</p> <h2 id="point-wisely-transformable-interpolations">Point-wisely Transformable Interpolations</h2> <p>We first define what we mean by pointwise transformability between two processes.</p> <blockquote class="definition"> <p><strong>Definition 1.</strong> Let \(\{X_t : t \in [0,1]\}\) and \(\{X'_t : t \in [0,1]\}\) be two interpolations. They are <strong>pointwise transformable</strong> if there exist differentiable maps \(\tau: [0,1] \to [0,1]\) and \(\phi: [0,1] \times \mathbb{R}^d \to \mathbb{R}^d\) such that \(\phi_t\) is invertible for each \(t \in [0,1],\) and</p> \[X'_t = \phi_t(X_{\tau_t}), \quad \forall t \in [0,1].\] </blockquote> <p>If two interpolations are pointwise transformable and contructed from the same coupling, their rectified flows are also related by the <strong>same</strong> transform, and they induce the same coupling.</p> <blockquote class="theorem"> <p><strong>Theorem 1.</strong> Suppose \(\{X_t\}\) and \(\{X'_t\}\) are pointwise transformable and constructed from the same coupling \((X_0, X_1) = (X'_0, X'_1)\). Assume \(\tau_0=0\) and \(\tau_1=1\).</p> <p>Let \(\{v_t\}\) and \(\{v'_t\}\) be the corresponding rectified flow velocity fields, and let \(\{Z_t\}\) and \(\{Z'_t\}\) their rectified flows with \(\mathrm d Z_t = v_t(Z_t)\mathrm d t,Z_0 = X_0\) and \(\mathrm d Z'_t = v'_t(Z'_t)\mathrm d t, Z'_0=X'_0.\) Then:</p> <ol> <li> <p>\(\{Z_t\},\{Z'_t\}\) can be transformed with the same pointwise maps:</p> \[Z'_t = \phi_t(Z_{\tau_t}) \quad \text{for all } t \in [0,1].\] </li> <li> <p>The rectified couplings match:</p> \[(Z_0, Z_1) = (Z'_0, Z'_1).\] </li> <li> <p>The velocity fields satisfy:</p> \[v'_t(x) = \partial_t \phi_t(\phi_t^{-1}(x)) + \bigl(\nabla \phi_t(\phi_t^{-1}(x))\bigr)^\top v_{\tau_t}(\phi_t^{-1}(x)) \dot{\tau}_t. \tag{1}\] </li> </ol> </blockquote> <p>This is equivalent to saying that the \(\texttt{Rectify}(\cdot)\) map is <strong>equivariant</strong> under the pointwise transforms \(\texttt{Transform}:\)</p> \[\texttt{Rectify}(\texttt{Transform}(\{X_t\})) = \texttt{Transform}(\texttt{Rectify}(\{X_t\})).\] <h3 id="affine-interpolations-are-pointwise-transformable">Affine Interpolations are Pointwise Transformable</h3> <p>Many commonly used interpolation schemes are affine \(X_t = \alpha_t X_1 + \beta_t X_0,\) with \(\alpha_t\) and \(\beta_t\) are monotone, \(\alpha_0=\beta_1=0,\) and \(\alpha_1 = \beta_0 = 1.\) Examples include:</p> <ol> <li> <p><strong><em>Straight interpolation</em></strong> <d-cite key="liu2022flow,lipman2022flow,albergo2023stochastic"></d-cite>:</p> \[X_t = tX_1 + (1-t) X_0.\] <p>This yields straight lines connecting \(\pi_0\) and \(\pi_1\) at a constant speed \(\dot X_t = X_1 - X_0.\)</p> </li> <li> <p><strong><em>Spherical linear interpolation</em></strong> (<em>slerp</em>) <d-cite key="nichol2021improved"></d-cite>:</p> \[X_t = \sin\left(\frac{\pi}{2} t\right)X_1 + \cos\left(\frac{\pi}{2} t\right)X_0,\] <p>which travels along the shortest great-circle arc on a sphere at a constant speed.</p> </li> <li> <p><strong><em>DDIM interpolation</em></strong> <d-cite key="song2020denoising"></d-cite> a spherical interpolation satisfying \(\alpha_t^2 + \beta_t^2 = 1\) but with a non-uniform speed defined by $\alpha_t$:</p> \[X_t = \alpha_t X_1 + \sqrt{1-\alpha_t^2} X_0,\] </li> </ol> <p>where \(\alpha_t = \exp\bigl(-\frac{1}{4}a(1-t)^2 - \tfrac{1}{2}b(1-t)\bigr)\), and \(a=19.9,b=0.1\) by default.</p> <p>For affine interpolations, the maps \(\phi\) and \(\tau\) reduce to scalar transforms: <strong>All affine interpolations are pointwise transformable by adjusting time and scaling</strong>. Hence, their induced rectified flows and couplings are equivalent. This result aligns with observations made by other authors<d-cite key="karras2022elucidating,kingma2024understanding,shaulbespoke,gao2025diffusionmeetsflow"></d-cite>.</p> <blockquote class="definition"> <p><strong>Proposition 1. Conversion of Affine Interpolations</strong></p> <p>Let \(X_t = \alpha_t X_1 + \beta_t X_0\) and \(X_t' = \alpha_t' X_1 + \beta_t' X_0\) be two affine interpolations from the same coupling \((X_0, X_1).\) Then there exist scalar functions \(\tau_t\) and \(\omega_t\) such that \(X_t' = \frac{1}{\omega_t} X_{\tau_t}, \quad \forall t \in [0,1],\)</p> <p>where \(\tau_t\) and \(\omega_t\) solve</p> \[\frac{\alpha_{\tau_t}}{\beta_{\tau_t}} = \frac{\alpha'_t}{\beta'_t}, \quad \omega_t = \frac{\alpha_{\tau_t}}{\alpha'_t} = \frac{\beta_{\tau_t}}{\beta'_t}, \quad \forall t \in (0, 1) \tag{2}\] <p>with the boundary conditions \(\omega_0 = \omega_1 = 1, \tau_0 = 0, \tau_1 = 1.\)</p> </blockquote> <p>In practice, we can determine \(\tau_t\) numerically—e.g., via a <a href="https://github.com/lqiang67/rectified-flow/blob/main/rectified_flow/flow_components/interpolation_convertor.py">binary search</a>—or derive an analytic solution in certain simple cases.</p> <div class="l-body"> <figure id="figure-1"> <div style="display: flex;"> <iframe src="/assets/plotly/interp_tau_ddim_spherical.html" frameborder="0" scrolling="no" height="330px" width="50%"></iframe> <iframe src="/assets/plotly/interp_tau_straight_spherical.html" frameborder="0" scrolling="no" height="330px" width="50%"></iframe> </div> <figcaption> <a href="#figure-1">Figure 1</a>. This figure shows the \(\tau\) and \(\omega\) transformations used to convert DDIM to spherical interpolation (left) and to convert straight interpolation to spherical (right). When converting DDIM to spherical, only the time scaling changes, as \(\omega_t\) remains fixed at 1. </figcaption> </figure> </div> <p>Combining Proposition 1 with Theorem 1, we have:</p> <blockquote class="definition"> <p><strong>Proposition 2. Rectified Flows between Affine Interpolation</strong></p> <p>For affine interpolations \(\{X_t\}\) and \(\{X'_t\}\):</p> <ul> <li>Their rectified flows \(\{Z_t\}\) and \(\{Z'_t\}\) satisfy:</li> </ul> \[Z'_t = \frac 1 {\omega_t} Z_{\tau_t}, \quad \forall t \in [0, 1].\] <ul> <li>Their rectified couplings match:</li> </ul> \[(Z_0, Z_1) = (Z'_0, Z'_1).\] <ul> <li>Their rectified flow velocity fields \(v_t\) and \(v'_t\) relate as:</li> </ul> \[v'_t(x) = \frac{1}{\omega_t} \left( \dot{\tau}_t v_{\tau_t}(\omega_t x) - \dot{\omega}_t x \right). \tag{3}\] </blockquote> <blockquote class="example"> <p><strong>Example 1. Velocity from Straight to Affine</strong></p> <p>For the straight interpolation \(X_t=tX_1 + (1-t)X_0\) with \(\alpha_t=t\) and \(\beta_t=1-t\). Convert it into another affine interpolation \(X'_t = \alpha'_t X_1 + \beta'_t X_0\) yields:</p> \[\tau_t = \frac{\alpha'_t}{\alpha'_t + \beta_t'}, \quad \omega_t = \frac{1}{\alpha_t' + \beta_t'}.\] <p>Their rectified flow velocity fields satisfy:</p> \[v'_t(x) = \frac{\dot{\alpha}'_t \beta'_t - \alpha'_t \dot{\beta}'_t}{\alpha'_t + \beta'_t} v_{\tau_t}(\omega_t x) \;+\; \frac{\dot{\alpha}'_t + \dot{\beta}'_t}{\alpha'_t + \beta'_t} x.\] </blockquote> <div class="l-body"> <figure id="figure-2"> <iframe src="/assets/plotly/interp_convert_200step.html" frameborder="0" scrolling="no" height="530px" width="100%"> </iframe> <figcaption> <a href="#figure-2">Figure 2</a>. Beginning with a rectified flow trained under straight interpolation, we convert it into a spherical rectified flow and apply Euler sampling to both. Although the two rectified flows follow different paths, they converge to the same endpoint \(Z_1\) because their continuous-time ODE trajectories are equivalent. </figcaption> </figure> </div> <h3 id="implication-on-inference">Implication on Inference</h3> <div class="l-body-outset"> <figure id="figure-3"> <div style="display: flex;"> <iframe src="/assets/plotly/interp_convert_10step_straight.html" frameborder="0" scrolling="no" height="330px" width="50%"></iframe> <iframe src="/assets/plotly/interp_convert_10step_spherical.html" frameborder="0" scrolling="no" height="330px" width="50%"></iframe> </div> <figcaption> <a href="#figure-3">Figure 3</a>. As we reduce the number of Euler steps to 4, the generated results become completely different. </figcaption> </figure> </div> <div class="l-body"> <figure id="figure-4"> <iframe src="/assets/plotly/interp_mse_step.html" frameborder="0" scrolling="no" height="310px" width="60%"> </iframe> <figcaption> <a href="#figure-4">Figure 4</a>. The Mean Square Error (MSE) between results generated by the two RFs decreases as the number of inference steps increases, indicating that their continuous ODEs produce the same results. However, different discretization schemes still lead to variations in performance. </figcaption> </figure> </div> <p>In theory, different affine interpolations induce equivalent continuous-time rectified flows. However, when we numerically solve ODEs, different interpolation schemes produce different discretization errors. Straighter trajectories generally yield smaller errors and more accurate final results.</p> <p>Fortunately, because we can convert between affine interpolation schemes without retraining, we can choose a scheme at inference time that yields “straighter” trajectories \(\{Z_t\}\), thus improving sampling performance.</p> <h2 id="implications-on-loss-functions">Implications on Loss Functions</h2> <p>Suppose we have trained a parametric model \(v_t(x;\theta)\) to approximate the RF velocity field \(v_t(x)\) under a particular affine interpolation. After training, we can use the relationships derived above to convert this model into \(v'_t(x;\theta)\), corresponding to a different affine interpolation, without retraining.</p> <p>This leads to two related questions:</p> <ol> <li>How does training with one interpolation differ from training directly with another?</li> <li>Does converting a pretrained model \(v_t\) to \(v'_t\) degrade performance?</li> </ol> <p>It turns out that choosing a different affine interpolation during training is equivalent to changing the <strong>time-weighting</strong> in the loss function and applying an affine transform to the model parameterization. As long as the transformations \(\omega_t\) and \(\tau_t\) are not highly singular, converting a model from one affine interpolation to another does not inherently reduce performance.</p> <p>Specifically, consider a model \(v_t(x; \theta)\) trained to approximate the RF velocity \(v_t\) of interpolation \(X_t = \alpha_t X_1 + \beta_t X_0\):</p> \[\mathcal L(\theta) = \int_0^1 \mathbb E\left[ \eta_t \left \| \dot X_t - v_t(X_t;\theta)\right\|^2 \right] \mathrm dt. \tag{4}\] <p>After training, we can convert this model \(v_t(x; \theta)\) into an approximation of \(v'_t\) of a different interpolation \(X'_t = \alpha_t' X_1 + \beta_t' X_0\) via:</p> \[v'_t(x; \theta) = \frac{\dot{\tau}_t}{\omega_t} v_{\tau_t}(\omega_t x; \theta) - \frac{\dot{\omega}_t}{\omega_t} x.\] <p>On the other hand, if we were to train \(v'_t(x; \theta)\) directly to approximate the velocity \(v'_t\) of interpolation \(X'_t = \alpha'_t X_1 + \beta'_t X_0\), the loss function is:</p> \[\mathcal L'(\theta) = \int_0^1 \mathbb{E} \left[ \eta'_t \left\| \dot{X}'_t - v'_t(X'_t; \theta) \right\|^2 \right] \mathrm dt \tag{5}\] <p>Matching the loss \((4)\) and \((5)\), shows that the two training schemes differ only in time-weighting and parameterization. Specifically,</p> \[\eta'_t = \frac{\omega_t^2}{\dot{\tau}_t} \eta_{\tau_t}, \quad v'_t(x; \theta) = \frac{\dot{\tau}_t}{\omega_t} v_{\tau_t}(\omega_t x; \theta) - \frac{\dot{\omega}_t}{\omega_t} x. \tag{6}\] <p>In other words, <strong>training under different affine interpolation schemes is equivalent to applying a different time-weighting function and a corresponding model reparameterization.</strong></p> <blockquote class="example"> <p><strong>Example 2. Loss from Straight to Affine</strong></p> <p>Consider the straight interpolation \(X_t = t X_1 + (1 - t) X_0\) and another affine interpolation \(X_t' = \alpha_t' X_1 + \beta_t' X_0.\)</p> <p>Suppose we have trained a model \(v_t\) for the straight interpolation with time weights \(\eta_t.\) Then \(v_t'\) converted from \(v_t\) corresponds to the RF trained with the parametrization in \((6)\), and a different time-weighting:</p> \[\eta_t' = \frac{\omega_t^2}{\tau_t'} \eta_{\tau_t} = \frac{1}{\dot{\alpha}_t' \beta_t' - \alpha_t' \dot{\beta}_t'} \eta_{\tau_t}.\] <p>Here, we substitute the relationships derived in Example 1 into \((6)\).</p> </blockquote> <h3 id="straight-vs-spherical-identical-train-time-weight">Straight vs. Spherical: Identical Train Time Weight</h3> <p>Following Example 2, an interesting case arises when \(\dot{\alpha}'_t \beta'_t - \alpha'_t \dot{\beta}'_t\) is a constant. In this case, \(\eta'_t\) is proportional to \(\eta_{\tau_t}\). If \(\eta_t = 1\) is uniform, then \(\eta'_t\) also remains uniform, meaning the two interpolation schemes share the same loss function.</p> <blockquote class="example"> <p><strong>Example 3. Losses for Straight vs. Spherical Interpolation</strong></p> <p>Consider the spherical interpolation: \(X'_t = \sin\left(\frac{\pi t}{2}\right)X_1 + \cos\left(\frac{\pi t}{2}\right)X_0.\)</p> <p>For this choice, \(\dot{\alpha}'_t \beta'_t - \alpha'_t \dot{\beta}'_t = \frac{\pi}{2}\). Hence:</p> \[\eta'_t = \frac{2}{\pi}\eta_{\tau_t}, \quad \tau_t = \frac{\tan\left(\frac{\pi t}{2}\right)}{\tan\left(\frac{\pi t}{2}\right)+1}.\] <p>Thus, training \(v_t\) for the straight interpolation with a uniform weight (\(\eta_t=1\)) is equivalent to training \(v'_t\) for the spherical interpolation with a uniform weight (\(\eta'_t=2/\pi\)). The only difference is a reparameterization of the model:</p> \[v'_t(x, \theta) = \frac{\pi \omega_t}{2} \left( v_{\tau_t}(\omega_t x; \theta) + \left( \cos\left(\frac{\pi t}{2}\right) - \sin\left(\frac{\pi t}{2}\right) \right) x \right),\] <p>where \(\omega_t = (\sin(\frac{\pi t}{2}) + \cos(\frac{\pi t}{2}))^{-1}\) is bounded within \([1/\sqrt{2}, 1]\).</p> </blockquote> <p>This reparameterization does not significantly affect performance. In the following figures, we show that choosing between straight or spherical interpolation makes <strong>little difference</strong> in training outcomes.</p> <div class="l-body"> <figure id="figure-5"> <iframe src="/assets/plotly/interp_convert_double_rf.html" frameborder="0" scrolling="no" height="530px" width="100%"> </iframe> <figcaption> <a href="#figure-5">Figure 5</a>. Comparing two RF models: one trained with straight interpolation and another trained with spherical interpolation, both uniform loss weight. We then convert the straight RF into a spherical form ("RF trained from straight"). Since both share the same loss function, the only difference lies in model parameterization. The results show only minor discrepancies, confirming that the reparameterization’s impact on performance is limited. </figcaption> </figure> </div>]]></content><author><name>Runlong Liao</name></author><category term="tutorial"/><summary type="html"><![CDATA[Affine Interpolations Result in Equivariant Rectified Flows]]></summary></entry><entry><title type="html">Flow to Diffusion: Langevin is a Guardrail</title><link href="https://rectifiedflow.github.io/blog/2024/samplers/" rel="alternate" type="text/html" title="Flow to Diffusion: Langevin is a Guardrail"/><published>2024-12-07T10:00:00+00:00</published><updated>2024-12-07T10:00:00+00:00</updated><id>https://rectifiedflow.github.io/blog/2024/samplers</id><content type="html" xml:base="https://rectifiedflow.github.io/blog/2024/samplers/"><![CDATA[ <h2 id="overview">Overview</h2> <p>Rectified flow (RF) yields an deterministic ODE (a.k.a. flow) model, of the form \(\mathrm d Z_t = v_t(Z_t)\mathrm d t\), which generates the data \(Z_1\) starting from an initial noise \(Z_0\). This approach offers a simplification compared to diffusion models that on a stochastic differential equation (SDE) to generate data from noise, such as DDPM and score-based models.</p> <p>However, the boundary between flow and diffusion models has been known to be blurry since the work of DDIM and probability-flow ODEs, which showed that it is possible to convert SDEs to ODEs during the post-training phase, without requiring re-training of the model. Now taking the ODE models, it is also possible to revert the process and convert the RF ODE to SDE to obtain stochastic samplers at inference time. Several questions arises:</p> <ol> <li> <p>Why and how is it possible to convert between SDEs and ODEs? What is the intuition?</p> </li> <li> <p>Why would we bother to add diffusion noise? What are the pros and cons?</p> </li> </ol> <p>We will explore these questions in this blog. For a more detailed discussion, see Chapter 5 of the <a href="https://github.com/lqiang67/rectified-flow/tree/main/pdf">Rectified Flow Lecture Notes</a>. Related works include DDIM<d-cite key="song2020denoising"></d-cite>, score-based SDEs<d-cite key="song2020score"></d-cite>, EDM<d-cite key="karras2022elucidating"></d-cite>.</p> <h2 id="stochastic-samplers--rf--langevin">Stochastic Samplers = RF + Langevin</h2> <p>Given a coupling \((X_0, X_1)\) of noise and data points, rectified flow defines an interpolation process, such as \(X_t = t X_1 + (1 - t) X_0\), and “rectifies” or “causalizes” it to yield an ODE model \(\mathrm{d} Z_t = v_t(Z_t) \, \mathrm{d} t\) initialized from \(Z_0 = X_0.\) The velocity field is given by \(v_t(z) = \mathbb{E}[\dot{X}_t \mid X_t = z],\) which is estimated by minimizing the loss \(\mathbb{E}_{t, X_0, X_1} [ \| \dot{X}_t - v_t(X_t) \|^2 ].\)</p> <p>The key property of RF ODE is the marginal preservation property: the distribution of \(Z_t\) on the ODE trajectory matches the distribution of \(X_t\) on the interpolation path at each time \(t\). This is ensured by the construction of the velocity field \(v_t\). As a result, the final output \(Z_1\) of the ODE follows the same distribution as \(X_1\), the target data distribution. This follows an inductive principle: if the distributions of \(X_t\) and \(Z_t\) match up to a given time, the construction of \(v_t\) ensures they will continue to match at the next (infinitesimal) step. By being “scheduled to do the right thing at the right time,” the process guarantees the correct final result.</p> <p>An obvious problem of this is that, in practice, errors can accumulate over time as we solve the ODE \(\mathrm{d} Z_t = v_t(Z_t) \mathrm{d} t\). These errors arise from model approximations and numerical discretization, causing drift between the estimated distribution and the true distribution. The issue can compound: if the estimated trajectory \(\hat{Z}_t\) deviates significantly from the distribution of \(X_t\), the update direction \(v_t(\hat{Z}_t)\) becomes less accurate. This happens because fewer data points are sampled in low-probability regions during training, where model inaccuracies are more pronounced.</p> <p>To address this problem, we may introduce a feedback mechanism to correct the error. One such approach is to use Langevin dynamics.</p> <p>Let \(\rho_t\) be the density function of \(X_t\), representing the true distribution that we aim to follow at time \(t\). At each time step \(t\), we can in principle apply a short segment of Langevin dynamics to adjust the trajectory’s distribution toward \(\rho_t\):</p> \[\mathrm{d} Z_{t, \tau} = \sigma_t^2 \nabla \log \rho_t(Z_{t, \tau}) \, \mathrm{d} \tau + \sqrt{2} \, \sigma_t \, \mathrm{d} W_\tau, \quad \tau \geq 0,\] <p>where \(\tau\) is a new time scale introduced for the Langevin dynamics, \(\sigma_t\) controls the noise level, and \(\nabla \log \rho_t\) adjusts the drift to steer the distribution toward high-probability regions of \(\rho_t\). It is well known that Langevin dynamics converge to the target distribution as \(\tau \to \infty\).</p> <p>Fully simulating Langevin dynamics would require a double-loop algorithm, where the system must be simulated to equilibrium (\(\tau\to\infty\)) at each time \(t\) before moving to the next time point.</p> <p>In rectified flow, however, the trajectory is already close to \(\rho_t\) at each time step \(t\). Therefore, a single step of Langevin dynamics can be sufficient to reduce the drift. This allows us to directly integrate Langevin corrections into the rectified flow updates, yielding a combined stochastic differential equation (SDE):</p> \[\mathrm{d}\tilde{Z}_t = \underbrace{v_t(\tilde{Z}_t) \mathrm{d} t}_{\text{Rectified Flow}} + \underbrace{\sigma_t^2 \nabla \log \rho_t(\tilde{Z}_t) \mathrm{d} t + \sqrt{2} \sigma_t \mathrm{d}W_t}_{\text{Langevin Dynamics}}, \quad \tilde{Z}_0 = Z_0.\] <p>This combined SDE achieves two key objectives:</p> <ol> <li> <p>The <strong>rectified flow</strong> drives the generative process forward as intended.</p> </li> <li> <p>The <strong>Langevin component</strong> acts as a negative feedback loop, correcting distributional drift without bias when \(\tilde{Z}_t\) and \(\rho_t\) are well aligned.</p> </li> </ol> <p>When the simulation is accurate, Langevin dynamics naturally remain in equilibrium, avoiding unnecessary changes to the distribution. However, if deviations occur, this mechanism guides the estimate back on track, enhancing the robustness of the inference process.</p> <p>The figure below illustrates the score function \(\nabla \log \rho_t\) along the SDE trajectories. We can see that \(\nabla \log \rho_t\) points toward high-density regions, and hence can guid trajectories back to areas of higher probability whenever deviations occur.</p> <div class="l-body" style="text-align:center;"> <img src="/assets/img/score_function_on_sde_traj.png" alt="cross" style="max-width:58%;"/> </div> <p>The figure below compares the results of two sampling methods. On the left, we show the result of the Euler discretization on the deterministic ODE applied to an insufficiently trained $v_t$ (due to early stopping), resulting in a significant presence of outliers. On the right, the Euler–Maruyama method is used to simulate the SDE, which effectively suppresses the outliers through the feedback mechanism introduced by the score functions.</p> <div class="l-body"> <img src="/assets/img/euler_sde_compare.png" alt="cross" style="max-width:100%;margin-bottom: 20px"/> </div> <p>This correction mechanism seems to have effect on state-of-the-art text-to-image generation as well. In this <a href="https://arxiv.org/abs/2411.19415">recent work</a><d-cite key="hu2024amo"></d-cite>, we find that stochastic samplers improves the text rendering qualities over deterministic samplers on SOTA models such as Flux – it allows makes generated images better reflect the text in the prompt. The figure below highlights this improvement: on the left, applying the stochastic sampler to the Flux model consistently outperforms the deterministic Euler sampler in text rendering performance across all step sizes. On the right, we present qualitative examples showcasing the enhanced text rendering quality achieved with the stochastic sampler.</p> <div class="l-body"> <img src="/assets/img/flux_text_rendering.png" alt="cross" style="max-width:100%;margin-bottom: 20px"/> </div> <h2 id="sdes-with-tweedies-formula">SDEs with Tweedie’s formula</h2> <p>In general, it may be necessary to estimate the score function \(\nabla \log \rho_t\) in addition to the RF velocity \(v_t\). However, in certain special cases, the score function can be estimated using \(v_t\), thereby eliminating the need to retrain an additional model. This approach enables a training-free conversion between ODEs and SDEs.</p> <p>Specifically, if the rectified flow is induced by an affine interpolation \(X_t = \alpha_t X_1 + \beta_t X_0\), where \(X_0\) and \(X_1\) are independent (i.e., \(X_0 \perp\!\!\!\perp X_1\)) and \(X_0\) follows a standard Gaussian distribution, then by Tweedie’s formula, we have</p> \[\nabla \log \rho_t(x) = -\frac{1}{\beta_t} \mathbb{E}[X_0 \mid X_t = x].\] <p>On the other hand, the RF velocity is given by</p> \[v_t(x) = \mathbb{E}[\dot{X}_t \mid X_t = x] = \mathbb{E}[\dot{\alpha}_t X_1 + \dot{\beta}_t X_0 \mid X_t = x],\] <p>where \(X_t = \alpha_t X_1 + \beta_t X_0\).</p> <p>Using this, we can express \(\mathbb{E}[X_0 \mid X_t = x]\) in terms of \(v_t(x)\) and obtain</p> \[\nabla \log \rho_t(x) = \frac{\alpha_t v_t(x) - \dot{\alpha}_t x }{\lambda_t \beta_t},\] <p>where \(\lambda_t = \dot{\alpha}_t \beta_t - \alpha_t \dot{\beta}_t\).</p> <p>As a result, the SDE takes the form</p> \[\mathrm d Z_t = v_t(Z_t)\mathrm d t + \gamma (\alpha_t v_t (x) - \dot \alpha_t x) \mathrm{d} t + \sqrt{2 \lambda_t \beta_t \gamma_t} \mathrm{d} W_t,\] <p>where we set \(\sigma_t^2 = \lambda_t \beta_t \gamma_t\).</p> <p>In the case straight interpolation \(X_t = t X_1 + (1-t)X_0\), we have \(\nabla \log \rho_t(x) = \frac{t v_t(x) - x}{1-t}\), yielding</p> \[\mathrm d Z_t = v_t(Z_t)\mathrm d t + \gamma (t v_t (x) - x) \mathrm{d} t + \sqrt{2 \gamma (1-t) } \mathrm{d} W_t.\] <p>The SDE of DDPM and the score-based SDEs is be recovered when \(\gamma_t = 1 / \alpha_t\) and \(\alpha^2_t + \beta_t^2 = 1\), yielding</p> \[\mathrm{d} Z_t = 2 v_t(Z_t) \, \mathrm{d} t - \frac{\dot{\alpha}_t}{\alpha_t} Z_t \, \mathrm{d} t + \sqrt{2 \frac{\dot \alpha_t}{\alpha_t}} \mathrm{d} W_t.\] <h2 id="diffusion-may-cause-over-concentration">Diffusion May Cause Over-Concentration</h2> <p>Although things work out nicely in theory, we need to be careful that the introduced score function \(\nabla \log \rho_t(x)\) itself has errors, and it may introduce undesirable effects if we rely on it too much (with a large \(\sigma_t\)). This is indeed the case in practice. As shown in the figure below, when we increase the noise magnitude \(\sigma_t\), the generated samples to cluster closer to the centers of the Gaussian modes.</p> <div class="l-body"> <img src="/assets/img/sde_concentrate_2d.png" alt="cross" style="max-width:100%;margin-bottom: 20px"/> </div> <p>So, large diffusion yields more concentrate results. This is rather counter-intuitive. Why is it the case?</p> <p>To see this, assume the estimated velocity field is \(\hat v_t \approx v_t\). The resulting estimated score function from Tweedie’s formula is</p> \[\nabla \log \hat \rho_t(x) = \frac{1}{\lambda_t \beta_t} \left( \alpha_t \hat v_t(x) - \dot{\alpha}_t x \right).\] <p>Because \(\beta_t\) must converge to 0 as \(t \to 1\), the estimated score function \(\nabla \log \hat{\rho}_t(x)\) is likely to diverge to infinity in this limit. This divergence leads to a significant overestimation of the true magnitude of \(\nabla \log \rho_t(x)\), which may be finite. As a result, the outputs of the SDE tend to become overly concentrated. This occurs because \(\nabla \log \rho_t(x)\) increasingly points toward the centers of mass clusters, or the local optima of the probability density.</p> <p>In other words, the Langevin guardrail may become <em>excessive</em>, causing over-concentration. Note that the deciding factor here is the score function \(\nabla \log \rho_t(x)\), not the introduction of noise, as one might initially assume. The noise component, as part of Langevin dynamics, merely compensates for the concentration effects induced by \(\nabla \log \rho_t(x)\), rather than being the primary driver of those effects.</p> <p>In the context of text-to-image generation, this over-concentration effect often produces overly smoothed images, which may appear cartoonish. Such over-smoothing eliminates fine details and high-frequency variations, resulting in outputs with a blurred appearance. The figure below illustrates these differences: samples generated using the Euler sampler exhibit more high-frequency details, as seen in the texture of the parrot’s feathers and the structure of the smoke.</p> <div class="l-body"> <img src="/assets/img/euler_sde_comp_flux.png" alt="cross" style="max-width:100%;margin-bottom: 20px"/> </div>]]></content><author><name>Xixi Hu</name></author><category term="tutorial"/><summary type="html"><![CDATA[How to derive stochastic samplers from flow? What are the pros and cons of diffusion?]]></summary></entry><entry><title type="html">Rectified Flow: An Introduction</title><link href="https://rectifiedflow.github.io/blog/2024/intro/" rel="alternate" type="text/html" title="Rectified Flow: An Introduction"/><published>2024-12-06T10:00:00+00:00</published><updated>2024-12-06T10:00:00+00:00</updated><id>https://rectifiedflow.github.io/blog/2024/intro</id><content type="html" xml:base="https://rectifiedflow.github.io/blog/2024/intro/"><![CDATA[ <h2 id="overview">Overview</h2> <p>This blog provides a brief introduction to rectified flow, based on Chapter 1 of these <a href="https://github.com/lqiang67/rectified-flow/tree/main/pdf">lecture notes</a>. For more introduction, please refer to the original papers<d-cite key="liu2022flow,liu2022rectified"></d-cite> and these <a href="https://www.cs.utexas.edu/~lqiang/rectflow/html/intro.html">blogs</a>.</p> <h2 id="problem-learning-flow-generative-models">Problem: Learning Flow Generative Models</h2> <p>Generative modeling can be formulated as finding a computational procedure that transforms a noise distribution, denoted by \(\pi_0\), into an unknown data distribution \(\pi_1\) observed through data. In flow models, this procedure is represented by an ordinary differential equation (ODE):</p> \[\dot{Z}_t = v_t(Z_t), \quad \forall t \in [0,1], \quad \text{starting from } Z_0 \sim \pi_0, \tag{1}\] <p>where \(\dot{Z}_t = \mathrm dZ_t / \mathrm dt\) denotes the time derivative, and the velocity field \(v_t(x) = v(x, t)\) is a learnable function to be estimated to ensure that \(Z_1\) follows the target distribution \(\pi_1\) when starting from \(Z_0 \sim \pi_0\). In this case, we say that the stochastic process \(Z = \{Z_t\}\) provides an (ODE) transport from \(\pi_0\) to \(\pi_1\).</p> <p>It is important to note that, in all but trivial cases, there exist <em>infinitely many</em> ODE transports from \(\pi_0\) to \(\pi_1\), provided that at least one such process exists. Thus, it is essential to be clear about which types of ODEs we should prefer.</p> <p>One option is to favor ODEs that are easy to solve at inference time. In practice, the ODEs are approximated using numerical methods, which typically construct piecewise linear approximations of the ODE trajectories. For instance, a common choice is Euler’s method:</p> \[\hat{Z}_{t+\epsilon} = \hat{Z}_t + \epsilon v_t(\hat{Z}_t), \quad \forall t \in \{0, \epsilon, 2\epsilon, \dots, 1\}, \tag{2}\] <p>where \(\epsilon &gt; 0\) is a step size. Varying the step size \(\epsilon\) introduces a trade-off between accuracy and computational cost: smaller \(\epsilon\) yields high accuracy but incurs a larger number of calculation steps. Therefore, we should seek ODEs that can be approximated accurately even with large step sizes.</p> <div class="l-body"> <figure id="figure-1" style="text-align: center; margin: 0 auto;"> <iframe src="/assets/plotly/intro_euler_method.html" frameborder="0" scrolling="no" height="310px" width="80%"> </iframe> <figcaption> <a href="#figure-1">Figure 1</a>. Illustration of error accumulation in Euler method trajectories, starting from various initial points and deviating from the true solution curve over time. </figcaption> </figure> </div> <p>The ideal scenario arises when the ODE follows straight-line trajectories, in which case Euler approximation yields zero discretization error regardless of the choice of step sizes. In such cases, the ODE, up to time reparameterization, should satisfy:</p> \[Z_t = t Z_1 + (1 - t) Z_0, \quad \implies \quad \dot{Z}_t = Z_1 - Z_0.\] <p>These ODEs, known as straight transports, enable fast generative models that can be simulated in a single step. We refer to the resulting pair \((Z_0, Z_1)\) as a straight coupling of \(\pi_0\) and \(\pi_1\). In practice, perfect straightness may not be achievable, but a goal we can aim for is to make the ODE trajectories as straight as possible to maximize computational efficiency.</p> <h2 id="rectified-flow">Rectified Flow</h2> <p>To construct a flow transporting \(\pi_0\) to \(\pi_1\), let us assume that we are given an arbitrary coupling \((X_0, X_1)\) of \(\pi_0\) and \(\pi_1\), from which we can obtain empirical draws. This can simply be the independent coupling with law \(\pi_0 \times \pi_1\), as is common in practice when we have access to independent samples from \(\pi_0\) and \(\pi_1\). The idea is that we are going to take \((X_0, X_1)\) and convert it to a better coupling generated by an ODE model, and optionally, we can go further to iteratively repeat this process to further enhance desired properties, such as straightness.</p> <p>Rectified flow works in the following ways:</p> <ul> <li> <p><strong>Build Interpolation:</strong></p> <p>We build an interpolation process \(\{X_t\} = \{X_t : t \in [0, 1]\}\) that smoothly interpolates between \(X_0\) and \(X_1\). Given the perference on straight trajectories, there seems no immediate reason to not use the canonical straight-line interpolation:</p> \[X_t = t X_1 + (1 - t) X_0.\] <p>Here the interpolation \(\{X_t\}\) is a stochastic process generated in an <strong>“anchor-and-bridge”</strong> way: we first sample the endpoints \(X_0\) and \(X_1\) and then sample the intermediate trajectory connecting them. Such processes are also known as bridge processes, where the intermediate values of \(X_t\) smoothly “bridge” the distribution between \(X_0\) and \(X_1\).</p> </li> <li> <p><strong>Marginal Matching:</strong></p> <p>By construction, the marginal distributions of \(X_0\) and \(X_1\) match the target distributions \(\pi_0\) and \(\pi_1\) through the interpolation process \(\{X_t\}\). However, \(\{X_t\}\) is not a causal ODE process like \(\dot{Z}_t = v_t(Z_t)\), which generate the output \(Z_1\) by evolving forward in time from \(Z_0\). Instead, generating \(X_t\) requires knowledge of both \(X_0\) and \(X_1\), rather than evolving solely from \(X_0\) as \(t\) increases.</p> <p>This issue can be resolved if we can convert \(\{X_t\}\) somehow into a causal ODE process while preserving the marginal distributions of \(X_t\) at each time \(t\). Note that since we only care about the output \(X_1\), we only need to match the marginal distributions. There is no need to match the trajectory-wise joint distribution.</p> </li> </ul> <p>Perhaps surprisingly, marginal matching can be achieved by simply training the velocity field \(v_t\) of the ODE model \(\dot{Z}_t = v_t(Z_t)\) to match the slope \(\dot{X}_t\) of the interpolation process via:</p> \[\min_v \int_0^1 \mathbb{E} \left[ \left\| \dot{X}_t - v_t(X_t) \right\|^2 \right] \mathrm dt. \tag{3}\] <p>The theoretical minimum is achieved by:</p> \[v_t^*(x) = \mathbb{E} \left[ \dot{X}_t \mid X_t = x \right],\] <p>which is the condition expectation of the slope \(\dot{X}_t\) for all the interpolation trajectories passing through a given point \(X_t = x\).</p> <p>With the straight interpolation \(X_t = t X_1+(1-t)X_0\), we have \(\dot{X}_t = X_1 - X_0\) by taking the derivative of \(X_t\) with respect to \(t\). It yields: \(\min_v \int_0^1 \mathbb{E} \left[ \| \dot{X}_t - v_t(X_t) \|^2 \right] \mathrm dt, \quad X_t = t X_1 + (1 - t) X_0.\)</p> <p>In practice, the optimization in (3) can be efficiently solved even for large AI models when \(v\) is parameterized as modern deep neural nets. This is achieved by leveraging off-the-shelf optimizers with stochastic gradients, computed by drawing pairs \((X_0, X_1)\) from data, sampling \(t\) uniformly in \([0, 1]\), and then computing the corresponding \((X_t, \dot{X}_t)\) using the interpolation formula.</p> <blockquote class="theorem"> <p><strong>Expectation Notation.</strong> Recall that a stochastic process \(X_t = X(t, \omega)\) is a measurable function of time \(t\) and a random seed \(\omega\) (with, say, distribution \(\mathbb{P}\)). In the case above, the end points are the random seed, i.e., \(\omega = (X_0, X_1)\). The slope is given by \(\dot{X}_t = \partial_t X(t, \omega)\), which is also a function of the same random seed. The expectation in the loss, written in full, is</p> \[\mathbb{E}_{\omega \sim \mathbb{P}} \left[ \left\| \partial_t X(t, \omega) - v_t(X(t, \omega)) \right\|^2 \right].\] <p>In writing, we often omit the random seed. Whenever we take the expectation, it averages out all random sources inside the brackets except for those explicitly included in the conditioning.</p> </blockquote> <div class="l-body"> <figure id="figure-2"> <iframe src="/assets/plotly/intro_rf_three_in_one.html" frameborder="0" scrolling="no" height="420px" width="105%"> </iframe> <figcaption> <a href="#figure-2">Figure 2</a>. Rectified flow between \(\pi_0\) and \(\pi_1\). Blue and pink lines are the trajectories colored based on which mode of they are associated with for visualization. </figcaption> </figure> </div> <p>We illustrate the intuition in Fig.2(a):</p> <ul> <li> <p>Different trajectories of the interpolation process \(\{X_t\}\) can intersect, causing multiple possible values of \(\dot{X}_t\) for the same point \(X_t\).</p> </li> <li> <p>In contrast, by defining an ODE \(\dot{Z}_t = v_t^*(Z_t)\), the update direction \(\dot{Z}_t\) at each point \(Z_t\) is uniquely determined by \(Z_t\).</p> </li> <li> <p>Hence in Fig.2(b), at these intersection points where the direction is otherwise uncertain, the ODE “derandomizes” the update direction by following the conditional expectation \(\displaystyle v_t^*(X_t) = \mathbb{E}[\dot{X}_t \mid X_t].\)</p> </li> </ul> <blockquote class="definition"> <p><strong>Rectified Flow.</strong> For any time-differential stochastic process \(\{X_t\} = \{X_t : t \in [0, 1]\}\), we call the ODE process:</p> \[\dot{Z}_t = v_t^*(Z_t) \quad \text{with} \quad v_t^*(x) = \mathbb{E} \left[ \dot{X}_t \mid X_t = x \right], \quad Z_0 = X_0\] <p>the <strong>rectified flow</strong> induced by \(\{X_t\}\). We denote it as:</p> \[\{Z_t\} = \texttt{Rectify}(\{X_t\}).\] </blockquote> <p>What makes rectified flow \(\{Z_t\}\) useful is that it preserves the marginal distributions of \(\{X_t\}\) at each point while resulting in a “better” coupling \((Z_0, Z_1)\) in terms of optimal transport:</p> <ol> <li> <p><strong>Marginal Preservation</strong></p> <p>The \(\{X_t\}\) and its rectified flow \(\{Z_t\}\) share the same marginal distributions at each time \(t \in [0, 1]\), that is:</p> \[\text{Law}(Z_t) = \text{Law}(X_t), \quad \forall t \in [0, 1],\] <p>where \(\text{Law}(X_t)\) denotes the probability distribution (or law) of random variable \(X_t\).</p> <div class="l-body"> <img src="/assets/img/flow_in_out.png" alt="cross" style="max-width:100%;"/> </div> </li> <li> <p><strong>Transport Cost</strong></p> <p>The start-end pairs \((Z_0, Z_1)\) from the rectified flow \(\{Z_t\}\) guarantee to yield no larger transport cost than \((X_0, X_1)\), simultaneously for all convex cost functions \(c\):</p> \[\mathbb{E} \left[ c(Z_1 - Z_0) \right] \leq \mathbb{E} \left[ c(X_1 - X_0) \right], \quad \forall \text{convex } c : \mathbb{R}^d \to \mathbb{R}.\] <div class="l-body"> <img src="/assets/img/flow_transport_cost.png" alt="cost" style="max-width:100%;"/> </div> </li> </ol> <h2 id="reflow">Reflow</h2> <p>While rectified flows tend to favor straight trajectories, they are not perfectly straight. As shown in Figure, the flow makes turns at intersection points of the interpolation trajectories \(\{X_t\}\). How can we further improve the flow to achieve straighter trajectories and hence speed up inference?</p> <p>A key insight is that the start-end pairs \((Z_0, Z_1)\) generated by rectified flow, called the <strong>rectified coupling</strong> of \((X_0, X_1)\), form a better and “straighter” coupling compared to \((X_0, X_1)\). This is because if we connect \(Z_0\) and \(Z_1\) with a new straight-line interpolation, it would yield fewer intersection points. Hence, training a new rectified flow based on this interpolation would result in straighter trajectories, leading to faster inference.</p> <p>Formally, we apply the \(\texttt{Rectify}(·)\) procedure recursively, yielding a sequence of rectified flows starting from \((Z_0^0, X_1^0) = (X_0, X_1)\):</p> \[\texttt{Reflow:} \quad \quad \{Z_t^{k+1}\} = \texttt{Rectify}(\texttt{Interp}(Z_0^k, Z_1^k)),\] <p>where \(\text{Interp}(Z_0^k, Z_1^k)\) denotes an interpolation process given \((Z_0^k, Z_1^k)\) as the endpoints. We call \(\{Z_t^k\}\) the \(k\)-th rectified flow, or simply the <strong>\(k\)-rectified flow</strong>, induced from \((X_0, X_1)\).</p> <p>This reflow procedure is proved to “straighten” the paths of rectified flows in the following sense: Define the following measure of straightness of \(\{Z_t\}\):</p> \[S(\{Z_t\}) = \int_0^1 \mathbb{E} \left[ \|Z_1 - Z_0 - \dot{Z}_t\|^2 \right] \mathrm dt,\] <p>where \(S(\{Z_t\})\) is a measure of the straightness of \(\{Z_t\}\), with \(S(\{Z_t\}) = 0\) corresponding to straight paths. Then it can be found in paper<d-cite key="liu2022flow"></d-cite> that</p> \[\mathbb{E}_{k \sim \text{Unif}(\{1, \dots, K\})} \left[S(\{Z_t^k\})\right] = \mathcal{O}(1 / K),\] <p>which suggests that the average of \(S(\{Z_t^k\})\) in the first \(K\) steps decay with an \(\mathcal{O}(1 / K)\) rate.</p> <p>Hence, we would obtain perfectly straight-line dynamics in the limit of \(k \to +\infty\). Note that reflow can begin from any coupling \((X_0, X_1)\), so it provides a general procedure for straightening and thus speeding up any given dynamics while preserving the marginals.</p> <p>As shown in Fig.2 (b), after applying the “Reflow” operation, the trajectories become starighter than the original rectified flow \(Z_t\).</p> <blockquote> <p><strong>Reflow and Shoftcut Learning.</strong> Intuitively, reflow resembles shortcut learning in humans: once we solve a problem for the first time, we learn to go directly to the solution, enabling us to solve it more quickly the next time.</p> </blockquote>]]></content><author><name>Runlong Liao</name></author><category term="tutorial"/><summary type="html"><![CDATA[Learning ODE Generative Models while Favoring Straightness]]></summary></entry></feed>