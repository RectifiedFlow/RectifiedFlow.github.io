<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.4">Jekyll</generator><link href="https://rectifiedflow.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://rectifiedflow.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-08-25T22:18:27+00:00</updated><id>https://rectifiedflow.github.io/feed.xml</id><title type="html">Let us Flow Together ‡ºÑ‡øê‡øîüöÄ</title><subtitle>Tutorials for Rectified Flow. </subtitle><entry><title type="html">Curved √ó Curved = Straight: DDIM is Straight RF</title><link href="https://rectifiedflow.github.io/blog/2024/DDIM-and-natural-euler/" rel="alternate" type="text/html" title="Curved √ó Curved=Straight: DDIM is Straight RF"/><published>2024-12-10T11:00:00+00:00</published><updated>2024-12-10T11:00:00+00:00</updated><id>https://rectifiedflow.github.io/blog/2024/DDIM-and-natural-euler</id><content type="html" xml:base="https://rectifiedflow.github.io/blog/2024/DDIM-and-natural-euler/"><![CDATA[ <p>We know that the continuous-time ODE of the denoising diffusion implicit model (DDIM) is rectified flow (RF) with a time-scaled spherical interpolation. However, the discrete inference rule of DDIM does not exactly apply the vanilla Euler method, \(\hat Z_{t+\epsilon } = \hat Z_t + \epsilon \cdot v_t(\hat Z_t)\), to the RF ODE \(\mathrm{d} Z_t = v_t(Z_t) \mathrm{d} t\). Instead, it uses a somewhat complicated rule:</p> \[\hat{Z}_{t+\epsilon} = \frac{\dot{\alpha}_t \beta_{t+\epsilon} - \alpha_{t+\epsilon} \dot{\beta}_t}{\dot{\alpha}_t \beta_t - \alpha_t \dot{\beta}_t} \hat{Z}_t + \frac{\alpha_{t+\epsilon} \beta_t - \alpha_t \beta_{t+\epsilon}}{\dot{\alpha}_t \beta_t - \alpha_t \dot{\beta}_t} v_t(\hat{Z}_t), \tag{1}\] <p>where $\alpha_t, \beta_t$ are the coefficients of the interpolation \(X_t = \alpha_t X_1 + \beta_t X_0\) that DDIM employs, which satisfy \(\alpha_t^2 + \beta_t^2 = 1\).</p> <p><strong>Question</strong>: <em>What is the nature of Equation (1) as a discretization technique for ODEs? How is it related to the vanilla Euler method?</em></p> <p>In this blog, we show that the DDIM inference is an instance of <em>natural Euler samplers</em>, which locally approximate ODEs using curved segments derived from the interpolation schemes employed during training. We also present a discrete-time extension of the equivariance result between pointwise transformable interpolations from <a href="https://rectifiedflow.github.io/blog/2024/interpolation/">blog</a>, showing that the natural Euler samplers of all affine interpolations are equivariant and produce (numerically) identical final outputs. Consequently, DDIM is, in fact, equivalent to the vanilla Euler method applied to a straight-line rectified flow.</p> <h2 id="natural-euler-samplers">Natural Euler Samplers</h2> <p>Rectified flow learns an ordinary differential equation (ODE) of the form \(\mathrm{d} Z_t = v_t(Z_t; \theta) \,\mathrm{d} t\) by matching its velocity field \(v_t(x)\) to the expected slope \(\mathbb{E}[\dot X_t \mid X_t=x]\) of an interpolation process \(\{X_t\}\) that connects the noise \(X_0\) and data \(X_1\). As discussed in a <a href="../interpolation/#affine-interpolations-are-pointwise-transformable">previous blog</a>, different affine interpolations \(X_t = \alpha_t X_1 + \beta_t X_0\) are pointwise transformable to one another and induce equivariant rectified flows with the same noise-data coupling.</p> <p>In practice, continuous-time ODEs must be solved numerically, with <strong>discrete solvers</strong>. A common approach is the Euler method, which approximates the flow \(\{Z_t\}\) on a discrete time grid \(\{t_i\}\) by:</p> \[\hat{Z}_{t_{i+1}} = \hat{Z}_{t_i} + (t_{i+1} - t_i) \cdot v_{t_i}(\hat{Z}_{t_i}),\] <p>which yields a discrete trajectory \(\{\hat{Z}_{t_i}\}_i\) composed of piecewise straight segments.</p> <p>The piecewise straight approximation is natural for rectified flows induced from straight-line interpolation, \(X_t = t X_1 + (1-t)X_0\). However, for a <em>curved</em> interpolation, it may be natural to approximate each step by a locally curved segment aligned with the corresponding interpolation process.</p> <div class="l-body"> <figure id="figure-1" style="margin: 1em auto;"> <div style="display: flex;"> <iframe src="/assets/plotly/discrete_vanilla_euler_4_step.html" frameborder="0" scrolling="no" height="250px" width="50%"></iframe> <iframe src="/assets/plotly/discrete_natural_euler_4_step.html" frameborder="0" scrolling="no" height="250px" width="50%"></iframe> </div> <figcaption> <a href="#figure-1">Figure 1</a>. The Vanilla Euler sampler (left) and the Natural Euler sampler (right) on spherical RF. The dashed lines show the ground truth ODE trajectories. The Vanilla Euler method approximates each step with a straight segment, whereas the Natural Euler method uses a locally curved segment derived from spherical interpolation. </figcaption> </figure> </div> <p>Specifically, given a general interpolation scheme \(X_t= \mathtt{I}_t(X_0, X_1)\), we update the trajectory along a curve segment defined by \(\mathtt{I}\):</p> \[\hat{Z}_{t_{i+1}} = \mathtt{I}_{t_{i+1}}(\hat{X}_{0 \mid t_i}, \hat{X}_{1 \mid t_i}),\] <p>where \(\hat{X}_{0 \mid t_i}\) and \(\hat{X}_{1 \mid t_i}\) are determined by identifying the interpolation curve that passes through \(\hat{Z}_{t_i}\) with the slope \(\partial_t \mathtt{I}_{t_i}(\hat{X}_{0 \mid t_i}, \hat{X}_{1 \mid t_i})\) matching \(v_{t_i}(\hat{Z}_{t_i})\). In other words, \(\hat{X}_{0 \mid t_i}\) and \(\hat{X}_{1 \mid t_i}\) are the solutions of the following equation:</p> \[\begin{cases} \hat{Z}_{t_i} = \mathtt{I}_{t_{i}}(\hat{X}_{0 \mid t_i}, \hat{X}_{1 \mid t_i}), \\[4px] v_t(\hat{Z}_{t_i}) = \partial_t \mathtt{I}_{t_{i}}(\hat{X}_{0 \mid t_i}, \hat{X}_{1 \mid t_i}). \tag{2} \end{cases}\] <p>We refer to this method as the <strong>natural Euler sampler</strong>.</p> <div class="l-gutter"> <img src="/assets/img/natural_euler.svg" style="max-width:200%"/> </div> <h3 id="natural-euler-samplers-for-affine-interpolations">Natural Euler Samplers for Affine Interpolations</h3> <p>For affine interpolations, \(X_t = \alpha_t X_1 + \beta_t X_0\), Equation (2) reduces to</p> \[\hat{Z}_{t} =\alpha_t\hat{X}_{1 \mid t} + \beta_t \hat{X}_{0 \mid t}, \quad v_t(\hat{Z}_{t}) =\dot{\alpha}_t\hat{X}_{1 \mid t} + \dot{\beta}_t \hat{X}_{0 \mid t}.\] <p>This gives</p> \[\hat X_{0|t} = \frac{-\alpha_t v_t(\hat Z_t) +\dot \alpha_t \hat Z_t}{\dot \alpha_t \beta_t - \alpha_t \dot \beta_t }, \quad \hat X_{1|t} = \frac{\beta_t v_t(\hat Z_t) - \dot \beta_t \hat Z_t}{\dot \alpha_t \beta_t - \alpha_t \dot \beta_t }.\] <p>Plugging it into \(\hat Z_{t+\epsilon} = \alpha_{t+\epsilon} \hat X_{1\mid t} + \beta_{t+\epsilon} \hat X_{0\mid t}\) yields the update rule in Equation (1).</p> <p>It might be tedious to mannually derive and handle equations like Equation (1) in practice. In our code base, we automatize the related derivations with <a href="https://github.com/lqiang67/rectified-flow/blob/main/rectified_flow/flow_components/interpolation_solver.py">affine interpolation solver</a>, which greatly simplifies the implementation process <a href="https://github.com/lqiang67/rectified-flow?tab=readme-ov-file#customized-samplers">code</a>.</p> <blockquote class="example"> <p><strong>Example 1. Natural Euler Sampler for Straight and Spherical Interpolation</strong></p> <p>For the straight intepolation \(X_t = t X_1 + (1-t) X_0\), it is easy to verify that it reduces the vanilla Euler sampler \(\hat{Z}_{t+\epsilon} = \hat{Z}_t + \epsilon v_t(\hat{Z}_t)\).</p> <p>For the time-uniform spherical interpolation \(X_t = \sin\left(\frac{\pi}{2}t\right) X_1 + \cos\left(\frac{\pi}{2} t\right) X_0\), the natural Euler update rule in Equation (1) reduces to</p> \[\hat Z_{t + \epsilon} =\cos\left(\frac{\pi}{2} \epsilon\right) \hat Z_{t} + \frac{2}{\pi} \sin \left(\frac{\pi}{2} \epsilon\right) v_t(\hat Z_t).\] </blockquote> <blockquote class="example"> <p><strong>Example 2. Natural Euler Sampler for DDIM</strong></p> <p>We verify that the update rule in Equation (1) coincides with the DDIM inference rule in <d-cite key="song2020denoising"></d-cite> when \(\alpha_t^2 + \beta_t^2 = 1\). Note that the inference update of DDIM in <d-cite key="song2020denoising"></d-cite> is written in terms of the expected noise \(\hat{x}_{0\mid t}(x) = \mathbb{E}[X_0 \mid X_t = x]\). Hence, we derive the update step using \(\hat{x}_{0 \mid t}\):</p> \[\begin{aligned} \hat{Z}_{t+\epsilon} &amp;= \alpha_{t+\epsilon} \cdot\hat{x}_{1\vert t}(\hat{Z}_t) + \beta_{t+\epsilon} \cdot \hat{x}_{0\vert t}(\hat{Z}_t) \\ &amp;\overset{*}{=} \alpha_{t+\epsilon} \left( \frac{\hat{Z}_t - \beta_t \cdot\hat{x}_{0\vert t}(\hat{Z}_t)}{\alpha_t} \right) + \beta_{t+\epsilon} \cdot \hat{x}_{0\vert t}(\hat{Z}_t) \\ &amp;= \frac{\alpha_{t+\epsilon}}{\alpha_t} \hat{Z}_t + \left( \beta_{t+\epsilon} - \frac{\alpha_{t+\epsilon} \beta_t}{\alpha_t} \right) \hat{x}_{0\vert t}(\hat{Z}_t) \end{aligned}\] <p>where in \(\overset{*}{=}\) we used \(\alpha_t \cdot \hat{x}_{1\vert t}(\hat{Z}_t) + \beta_t\cdot \hat{x}_{0\vert t}(\hat{Z}_t) = \hat{Z}_t\).</p> <p>We can slightly rewrite the update as:</p> \[\frac{\hat{Z}_{t+\epsilon}}{\alpha_{t+\epsilon}} = \frac{\hat{Z}_t}{\alpha_t} + \left( \frac{\beta_{t+\epsilon}}{\alpha_{t+\epsilon}} - \frac{\beta_t}{\alpha_t} \right) \hat{x}_{0\vert t}(\hat{Z}_t),\] <p>which precisely matches Equation (13) of <d-cite key="song2020denoising"></d-cite>.</p> </blockquote> <h2 id="equivalence-of-natural-euler-trajectories">Equivalence of Natural Euler Trajectories</h2> <p>A key feature of natural Euler sampling is that it <em>preserves</em> pointwise equivalences which we established in the continuous-time case. In other words, if two interpolations \(\{X_t\}\) and \(\{X_t'\}\) are related by a pointwise transform, then their corresponding <strong>discrete</strong> trajectories under natural Euler remain related by the <em>same</em> transform‚Äîprovided the time grids are properly scaled.</p> <blockquote class="theorem"> <p><strong>Theorem 1. Equivalence of Natural Euler Trajectories</strong></p> <p>Suppose \(\{X_t\}\) and \(\{X_t'\}\) are two interpolation processes contructed from the same couping, related by a pointwise transform \(X_t' = \phi_t(X_{\tau_t})\). Let \(\{\hat{Z}_{t_i}\}_i\) and \(\{\hat{Z}_{t_i'}'\}_i\) be discrete-time trajectories produced by the natural Euler samplers of the rectified flows induced by \(\{X_t\}\) and \(\{X_t'\}\) on time grids \(\{t_i\}\) and \(\{t_i'\}\), respectively.</p> <p>If \(\tau(t_i') = t_i\) for all \(i\), and the initial conditions align via \(\hat{Z}_{t_0'}' = \phi_{t_0'}(\hat{Z}_{\tau(t_0')})\), then the discrete trajectories are also related by the same transform:</p> \[\hat{Z}_{t_i'}' = \phi_{t_i'}(\hat{Z}_{t_i}) \quad \text{for all } i = 0,1,\ldots\] <p>In particular, for affine interpolations, if \(\hat{Z}_0 = \hat{Z}_0'\), then \(\hat{Z}_1 = \hat{Z}_1'\) <em>even if</em> the intermediate trajectories differ step by step.</p> </blockquote> <p>Let \(\{X_t'\} = \texttt{Transform}(\{X_t\})\) denote a pointwise transformation between two interpolations, and \(\{\hat{Z}_{t_i}\}=\texttt{NaturalEulerRF}(\{X_t\})\) be the natural Euler trajectory, and \(\hat{Z}_1=\texttt{NaturalEulerRFZ}_1(\{X_t\})\) the resulting final prediction of \(Z_1\). Then \(\texttt{NaturalEulerRF}\) is equivariant under \(\texttt{Transform}\), while \(\texttt{NaturalEulerRFZ}_1\) is equivalent:</p> \[\texttt{NaturalEulerRF}(\texttt{Transform}(\{X_t\})) = \texttt{Transform}(\texttt{NaturalEulerRF}(\{X_t\})),\] \[\texttt{NaturalEulerRFZ}_1(\texttt{Transform}(\{X_t\})) =\texttt{NaturalEulerRFZ}_1(\{X_t\}).\] <p>For two affine interpolations \(X_t = \alpha_t X_1 + \beta_t X_0\) and \(X_t' = \alpha_t' X_1 + \beta_t' X_0\), the transform can be realized by a time and variable scaling:</p> \[X_t' = \frac{1}{\omega_t} X_{\tau_t}, \quad \forall t \in [0,1],\] <p>where \(\tau_t\) and \(\omega_t\) solve \(\frac{\alpha_{\tau_t}}{\beta_{\tau_t}} = \frac{\alpha'_t}{\beta'_t},\) and \(\omega_t = \frac{\alpha_{\tau_t}}{\alpha'_t} = \frac{\beta_{\tau_t}}{\beta'_t}.\)</p> <p>Therefore, the natural Euler samplers (including DDIM) on the RF of affine interpolations are equivalent to one another upto the transform and time scaling. In particular, they are equivalent to the <strong>vanilla (straight) Euler sampler on the RF of the straight interpolation</strong>.</p> <blockquote class="example"> <p><strong>Example 3. Equivalence of Straight Euler and DDIM sampler</strong></p> <p>Consider the natural Euler update rule in Equation (1) applied to a RF induced by an affine interpolation \(\{X_t' = \alpha'_t X_1 + \beta'_t X_0\}\) using a uniform time grid \(t'_i = i/n\). This procedure is equivalent to applying the standard (straight) Euler method to the straight RF (induced by \(\{X_t = tX_1 + (1 - t)X_0\}\)), but with a non-uniform time grid:</p> \[t_i = \frac{\alpha'_{i/n}}{\alpha'_{i/n}+\beta'_{i/n}}.\] <p>Conversely, starting from the straight RF and applying the standard Euler sampler on \(\{t_i\}\), one can recover the natural Euler sampler for the affine interpolation by selecting a time grid \(\{t'_i\}\) such that:</p> \[t_i = \frac{\alpha'_{t'_{i}}}{\alpha'_{t'_{i}}+\beta'_{t'_{i}}}.\] <p>Since DDIM is the natural Euler sampler under spherical interpolation, and the straight-interpolation counterpart corresponds to the standard Euler method, appropriately scaling the time grid for the straight RF reproduces the results of the DDIM sampler exactly.</p> </blockquote> <div class="l-body"> <figure id="figure-2" style="margin: 1em auto;"> <div style="display: flex;"> <iframe src="/assets/plotly/discrete_euler_match_t.html" frameborder="0" scrolling="no" height="250px" width="50%"></iframe> <iframe src="/assets/plotly/discrete_natural_euler_match_t.html" frameborder="0" scrolling="no" height="250px" width="50%"></iframe> </div> <figcaption> <a href="#figure-2">Figure 2</a>. Running natural Euler samplers on both straight (left) and spherical (right) RFs. By adjusting the straight RF's time grid via \(\tau_t\), while keeping a uniform grid for the Spherical RF, we obtain (exactly) identical final results. Furthermore, their intermediate trajectories can also be aligned point by point under their corresponding interpolation transforms (not shown). </figcaption> </figure> </div>]]></content><author><name>Rectified Flow Group</name></author><category term="tutorial"/><summary type="html"><![CDATA[The discretized inference scheme of DDIM corresponds to a curved Euler method on curved trajectories, and is equivalent to the vanilla Euler method applied to straight rectified flow. But the latter is simpler...]]></summary></entry><entry><title type="html">Interpolations: All Flows are One Flow</title><link href="https://rectifiedflow.github.io/blog/2024/interpolation/" rel="alternate" type="text/html" title="Interpolations: All Flows are One Flow"/><published>2024-12-10T10:00:00+00:00</published><updated>2024-12-10T10:00:00+00:00</updated><id>https://rectifiedflow.github.io/blog/2024/interpolation</id><content type="html" xml:base="https://rectifiedflow.github.io/blog/2024/interpolation/"><![CDATA[ <p>Most diffusion and flow models can be analyzed through the rectified flow lens, but they employ different interpolation methods, typically affine interpolations such as straight-line or spherical interpolations. A critical question is to understand the impact of using different interpolation processes. This blog introduces the equivalent relationships between rectified flows induced by these different interpolation processes, as discussed in Chapter 3 of these <a href="https://www.cs.utexas.edu/~lqiang/PDF/flow_book.pdf">lecture notes</a>. Related observations and discussions can also be found in <d-cite key="karras2022elucidating,kingma2024understanding,shaulbespoke,gao2025diffusionmeetsflow"></d-cite>.</p> <h2 id="overview">Overview</h2> <p>Given a coupling \((X_0, X_1)\) of the noise \(X_0 \sim \pi_0\) and data \(X_1 \sim \pi_1\), in rectified flow, we leverage an interpolation process \(X_t = \mathtt{I}_t(X_0, X_1)\), \(t \in [0,1]\), to smoothly connect \(X_0\) and \(X_1\), and then ‚Äúcausalize‚Äù or ‚Äúrectify‚Äù the interpolation \(\{X_t\}\) into its rectified flow \(\{Z_t\} = \mathtt{Rectify}(\{X_t\})\), an ODE-based generative model of the form:</p> \[\mathrm{d}Z_t = v_t(Z_t) \mathrm{d}t, \quad Z_0 = X_0, \quad \text{with velocity field} \quad v_t(x) = \mathbb{E}[\dot{X}_t \mid X_t = x],\] <p>where \(\dot{X}_t\) is the time derivative of \(X_t\). This formulation of \(v_t\) ensures that \(Z_t\) matches in distribution with \(X_t\) at every time \(t\). With this, we can generate data as \(Z_1\) by evolving forward in time from noise \(Z_0\). Intuitively, the rectified flow \(\{Z_t\}\) ‚Äúrewires‚Äù the trajectories of \(\{X_t\}\) at their intersection points to produce non-intersecting ODE trajectories. For further details, see <a href="https://arxiv.org/abs/2209.03003">paper</a><d-cite key="liu2022flow"></d-cite>, <a href="https://www.cs.utexas.edu/~lqiang/rectflow/html/intro.html">blog</a>, <a href="https://rectifiedflow.github.io/blog/2024/intro/">blog</a>.</p> <p>In principle, any time-differentiable interpolation process \(\{X_t\}\) that connects \(X_0\) and \(X_1\) can be used within this framework. Different methods employ different interpolation schemes. The simplest choice is the straight-line interpolation, defined as</p> \[X_t = t X_1 + (1-t) X_0,\] <p>which is naturally justified by optimal transport theory.</p> <p>Alternatively, other methods, such as DDIM and probability-flow ODEs, use curved interpolation schemes of a more general affine form:</p> \[X_t = \alpha_t X_1 + \beta_t X_0,\] <p>where \(\alpha_t\) and \(\beta_t\) are chosen in different ways depending on the method.</p> <p><strong>Questions:</strong> <em>What is the impact of the choice of interpolation? Do different interpolation schemes yield fundamentally different rectified flow dynamics?</em></p> <p>At first glance, it may appear that the interpolation process must be chosen during the training phase, as it directly affects the learned rectified flow. However, this is not necessarily the case.</p> <p>As it turns out, if two interpolation processes can be <strong>deformed</strong> into each other with a differentiable pointwise transform (i.e., they are diffeomorphic in mathy terms), then the trajectories of their rectified flows can also be deformed into each other using <strong>the very same transform</strong>. In addition, if the two processes are constructed from the same couplings, then their rectified flows lead to the same rectified coupling \((Z_0, Z_1).\)</p> <p><em>Why is this true?</em> The intuition is illustrated in Figure 1. The trajectories of the rectified flow (RF) are simply a ‚Äúrewiring‚Äù of the interpolation trajectories at their intersection points to avoid crossings. As a result, they occupy the same ‚Äútrace‚Äù as the interpolation process, even though they switch between different trajectories at intersection points. Consequently, any deformation applied to the interpolation trajectories is inherited by the rectified flow trajectories. The deformation must be <strong>point-to-point</strong> here to make it insenstive to the rewiring of the trajectories.</p> <p>This is a general and fundamental property of the rectification process and is not restricted to specific distributions, couplings, or interpolations.</p> <figure id="figure-1" style="margin: 0 auto 1em auto;"> <div style="display: flex; justify-content: center;"> <img src="/assets/img/interpolation_conversion.gif" alt="interpolation conversion gif" style="max-width: 600px; height: auto;"/> </div> <figcaption> <a href="#figure-1">Figure 1</a>. Rectified flow (right) rewires the interpolation trajectories (left) to avoid crossing. When a deformation is applied on the interpolation trajectories, the trajectories of the corresponding rectified flow is deformed in the same way. </figcaption> </figure> <p>Notably, all affine interpolation processes \(X_t = \alpha_t X_1 + \beta_t X_0\) can be pointwisely transformed into one another through simple time and variable scaling. This suggests that, in principle, it is sufficient to use the simplest straight-line interpolation during training, while recovering the rectified flow for all affine interpolations at inference time.</p> <p>This analytic relation allows us to analyze the impact of training and inference under different interpolations. For training, using different affine interpolations corresponds to applying time weightings in the training loss. We analyze this for the common straight-line and cosine interpolations and find that it appears to have limited impact on performance. For inference, using different interpolations corresponds to applying numerical discretization on deformed ODE trajectories, which is discussed in depth in this <a href="https://rectifiedflow.github.io/blog/2024/discretization/">blog</a>.</p> <h2 id="point-wisely-transformable-interpolations">Point-wisely Transformable Interpolations</h2> <p>We first formalize <em>pointwise transformability</em> between two interpolation processes.</p> <blockquote class="definition"> <p><strong>Definition 1.</strong> Let \(\{X_t : t \in [0,1]\}\) and \(\{X'_t : t \in [0,1]\}\) be two interpolations. They are <strong>pointwise transformable</strong> if there exist differentiable maps</p> \[\tau: [0,1] \to [0,1] \quad \text{ and } \quad \phi: [0,1] \times \mathbb{R}^d \to \mathbb{R}^d\] <p>such that each \(\phi_t\) is invertible, and</p> \[X'_t = \phi_t(X_{\tau_t}), \quad \forall t \in [0,1].\] </blockquote> <div class="l-gutter"> <img src="/assets/img/interpolation/interpolation.svg" style="max-width:100%;"/> </div> <p>If two interpolations are contructed from the same coupling \((X_0, X_1)\) and are pointwise transformable, then their rectified flows are also related by the <strong>same</strong> transform, and also lead to the same rectified coupling.</p> <blockquote class="theorem"> <p><strong>Theorem 1.</strong> Suppose \(\{X_t\}\) and \(\{X'_t\}\) constructed from the same coupling \((X_0, X_1) = (X'_0, X'_1)\) and are pointwise transformable. Assume \(\tau_0=0\) and \(\tau_1=1\).</p> <p>Let \(\{Z_t\}\) and \(\{Z'_t\}\) be their rectified flows:</p> \[\{Z_t\} = \mathtt{Rectify}(\{X_t\}),~~~~~~ \{Z'_t\} = \mathtt{Rectify}(\{X'_t\}).\] <ol> <li> <p>\(\{Z_t\},\{Z'_t\}\) can be transformed with the same pointwise maps:</p> \[Z'_t = \phi_t(Z_{\tau_t}) \quad \forall t \in [0,1].\] </li> <li> <p>Their rectified couplings are the same: \((Z_0, Z_1) = (Z'_0, Z'_1).\)</p> </li> <li> <p>Let \(\{v_t\}\) and \(\{v'_t\}\) be the velocity fields of the rectified flows \(\{Z_t\}\) and \(\{Z_t'\}\), respectively. We have</p> \[v'_t(x) = \partial_t \phi_t(\phi_t^{-1}(x)) + \bigl(\nabla \phi_t(\phi_t^{-1}(x))\bigr)^\top v_{\tau_t}(\phi_t^{-1}(x)) \dot{\tau}_t. \tag{1}\] </li> </ol> </blockquote> <p>This is equivalent to saying that the \(\{Z_t\} = \texttt{Rectify}(\{X_t\})\) map is <strong>equivariant</strong> under the pointwise transforms \(\{X_t'\}=\texttt{Transform}(\{X_t\})\), while the the rectified coupling map \((Z_0,Z_1) = \mathtt{RectifyCoupling}(\{X_t\})\) is <strong>equivalent</strong>:</p> \[\texttt{Rectify}(\texttt{Transform}(\{X_t\})) = \texttt{Transform}(\texttt{Rectify}(\{X_t\})),\] \[\texttt{RectifyCoupling}(\texttt{Transform}(\{X_t\})) = \texttt{RectifyCoupling}(\{X_t\}).\] <figure id="figure-1" style="margin: 0 auto 1em auto;"> <div style="display: flex; justify-content: center;"> <img src="/assets/img/interpolation_conversion_illustration.svg" alt="interpolation conversion illustration" style="max-width: 600px; height: auto;"/> </div> </figure> <h3 id="affine-interpolations-are-pointwise-transformable">Affine Interpolations are Pointwise Transformable</h3> <p>Many commonly used interpolation schemes are affine $X_t = \alpha_t X_1 + \beta_t X_0,$ with \(\alpha_t\) and \(\beta_t\) are monotone, \(\alpha_0=\beta_1=0,\) and \(\alpha_1 = \beta_0 = 1.\) Examples include:</p> <ol> <li> <p><strong><em>Straight interpolation</em></strong> <d-cite key="liu2022flow,lipman2022flow,albergo2023stochastic"></d-cite>:</p> \[X_t = tX_1 + (1-t) X_0.\] <p>This yields straight lines connecting \(\pi_0\) and \(\pi_1\) at constant speed \(\dot X_t = X_1 - X_0.\)</p> </li> <li> <p><strong><em>Spherical linear interpolation</em></strong> (<em>slerp</em>) <d-cite key="nichol2021improved"></d-cite>:</p> \[X_t = \sin\left(\frac{\pi}{2} t\right)X_1 + \cos\left(\frac{\pi}{2} t\right)X_0,\] <p>which traces a shortest great-circle arc on a sphere at constant speed.</p> </li> <li> <p><strong><em>DDPM/DDIM interpolation</em></strong> <d-cite key="song2020denoising"></d-cite> A spherical interpolation satisfying \(\alpha_t^2 + \beta_t^2 = 1\) but with a non-uniform speed defined by $\alpha_t$:</p> \[X_t = \alpha_t X_1 + \sqrt{1-\alpha_t^2} X_0,\] <p>where \(\alpha_t = \exp\bigl(-\frac{1}{4}a(1-t)^2 - \tfrac{1}{2}b(1-t)\bigr)\), and \(a=19.9,b=0.1\) by default.</p> </li> </ol> <p><strong>All affine interpolations are pointwise transformable by adjusting time and scaling</strong>. In this case, the maps \(\phi\) and \(\tau\) reduce to scalar transforms, as observed in a line of works <d-cite key="karras2022elucidating,kingma2024understanding,shaulbespoke,gao2025diffusionmeetsflow"></d-cite>. Hence, the rectified flows of all affine interpolations can be analytically transformed into one another, and they lead to the same rectified couplings.</p> <blockquote class="definition"> <p><strong>Proposition 1. Pointwise Transforms Between Affine Interpolations</strong></p> <p>Let \(X_t = \alpha_t X_1 + \beta_t X_0\) and \(X_t' = \alpha_t' X_1 + \beta_t' X_0\) be two affine interpolations from the same coupling \((X_0, X_1).\) Then there exist scalar functions \(\tau_t\) and \(\omega_t\) such that</p> \[X_t' = \frac{1}{\omega_t} X_{\tau_t}, \quad \forall t \in [0,1], \tag{2}\] <p>where \(\tau_t\) and \(\omega_t\) solve</p> \[\frac{\alpha_{\tau_t}}{\beta_{\tau_t}} = \frac{\alpha'_t}{\beta'_t}, \quad \omega_t = \frac{\alpha_{\tau_t}}{\alpha'_t} = \frac{\beta_{\tau_t}}{\beta'_t}, \quad \forall t \in (0, 1)\] <p>with the boundary conditions \(\omega_0 = \omega_1 = 1, \tau_0 = 0, \tau_1 = 1.\)</p> </blockquote> <p>In practice, we can determine \(\tau_t\) numerically‚Äîe.g., via a <a href="https://github.com/lqiang67/rectified-flow/blob/main/rectified_flow/flow_components/interpolation_convertor.py">binary search</a>‚Äîor derive an analytic solution in certain simple cases.</p> <div class="l-body"> <figure id="figure-2" style="margin: 1em auto;"> <div style="display: flex;"> <iframe src="/assets/plotly/interp_tau_ddim_spherical.html" frameborder="0" scrolling="no" height="330px" width="50%"></iframe> <iframe src="/assets/plotly/interp_tau_straight_spherical.html" frameborder="0" scrolling="no" height="330px" width="50%"></iframe> </div> <figcaption> <a href="#figure-2">Figure 2</a>. The transforms \(\tau\) and \(\omega\) in Equation (2) that convert DDIM to spherical interpolation (left) and convert straight interpolation to spherical (right). When converting DDIM to spherical, \(\omega_t\) remains fixed at 1, because only the time scaling changes. </figcaption> </figure> </div> <p>Combining Proposition 1 with Theorem 1, we have:</p> <blockquote class="definition"> <p><strong>Proposition 2. Rectified Flows between Affine Interpolations</strong></p> <p>For the affine interpolations \(\{X_t\}\) and \(\{X'_t\}\) in Proposition 1, we have</p> <ul> <li>Their rectified flows \(\{Z_t\}\) and \(\{Z'_t\}\) satisfy:</li> </ul> \[Z'_t = \frac 1 {\omega_t} Z_{\tau_t}, \quad \forall t \in [0, 1].\] <ul> <li> <p>Their rectified couplings are identical: \((Z_0, Z_1) = (Z'_0, Z'_1).\)</p> </li> <li> <p>Their rectified flow velocity fields \(v_t\) and \(v'_t\) relate via:</p> </li> </ul> \[v'_t(x) = \frac{\dot{\tau}_t}{\omega_t} v_{\tau_t}(\omega_t x) - \frac{\dot{\omega}_t}{\omega_t} x. \tag{3}\] </blockquote> <blockquote class="example"> <p><strong>Example 1. Velocity from Straight to Affine</strong></p> <p>Converting the straight interpolation \(X_t=tX_1 + (1-t)X_0\) with \(\alpha_t=t\) and \(\beta_t=1-t\) into another affine interpolation \(X'_t = \alpha'_t X_1 + \beta'_t X_0\) gives:</p> \[\tau_t = \frac{\alpha'_t}{\alpha'_t + \beta_t'}, \quad \omega_t = \frac{1}{\alpha_t' + \beta_t'}.\] <p>Their rectified flow velocity fields satisfy:</p> \[v'_t(x) = \frac{\dot{\alpha}'_t \beta'_t - \alpha'_t \dot{\beta}'_t}{\alpha'_t + \beta'_t} v_{\tau_t}(\omega_t x) \;+\; \frac{\dot{\alpha}'_t + \dot{\beta}'_t}{\alpha'_t + \beta'_t} x.\] </blockquote> <div class="l-body"> <figure id="figure-3" style="margin: 0em auto;"> <iframe src="/assets/plotly/interp_convert_200step.html" frameborder="0" scrolling="no" height="430px" width="75%"> </iframe> <figcaption> <a href="#figure-3">Figure 3</a>. We first train a rectified flow using straight interpolation, and then transform it into the RF of spherical interpolation by applying the transformation formula described above. While the transformation results in different ODE trajectories, both ultimately converge to the same endpoints \(Z_1\), as predicted by Proposition 2. The result is obtained by solving the ODE using 100 Euler steps. </figcaption> </figure> </div> <h3 id="implication-on-inference">Implication on Inference</h3> <p>The trajectories of the RF derived from different affine interpolations can be viewed as deformations of one another via time and space scaling. When the same numerical discretization methods, such as the Euler method, are applied to these differently deformed trajectories, they produce varying discretization errors, leading to numerically different estimations of \(Z_1\) . This difference becomes pronounced when a large step size is used, as it introduces significant discretization errors; see <a href="#figure-4">Figure 4</a> for an example of the Euler method with 4 steps. However, the difference diminishes as the discretization becomes sufficiently fine to accurately approximate the underlying ODEs (as shown in <a href="#figure-3">Figure 3</a> with 100 Euler steps).</p> <p><a href="#figure-5">Figure 5</a> illustrates how the difference in the predicted outcome \(Z_1\) of the RF ODEs corresponding to straight and spherical interpolation decreases as the number of Euler steps increases.</p> <div class="l-body"> <figure id="figure-4" style="margin: 0em auto;"> <div style="display: flex;"> <iframe src="/assets/plotly/interp_convert_10step_straight.html" frameborder="0" scrolling="no" height="330px" width="50%"></iframe> <iframe src="/assets/plotly/interp_convert_10step_spherical.html" frameborder="0" scrolling="no" height="330px" width="50%"></iframe> </div> <figcaption> <a href="#figure-4">Figure 4</a>. Different final generated samples when the number of Euler steps is reduced to 4. </figcaption> </figure> </div> <div class="l-body"> <figure id="figure-5" style="margin: 1em auto;"> <div style="display: flex; justify-content: center;"> <iframe src="/assets/plotly/interp_mse_step.html" frameborder="0" scrolling="no" height="310px" width="60%"> </iframe> </div> <figcaption> <a href="#figure-5">Figure 5</a>. The mean square error (MSE) between the estimtion of \(Z_1\) from rectified flows induced from straight versus spherical interpolation decreases as the number of inference steps increases, reflecting their shared continuous-time limit. Nevertheless, different discretization schemes produce varying performance when the step count is small. </figcaption> </figure> </div> <p>In general, we may want to reduce these errors by seeking ‚Äústraighter‚Äù trajectories when the Euler method is used for discretization. Note, however, if ‚Äúcurved‚Äù variants of the Euler method are employed, the notion of straightness must be adapted to account for the curvature inherent in the curved Euler method. For further discussion, refer to <a href="https://rectifiedflow.github.io/blog/2024/discretization/">this blog</a>.</p> <p>Although it is challenging to predict the best inference interpolation scheme <em>a priori</em>, the post-training conversion above allows us to choose whichever scheme that yields best sampling results in practice. Moreover, one can go a step further by directly optimizing the pointwise transform to minimize discretization error, <em>without worrying about which interpolation scheme it corresponds to</em>. Specifically, this involves directly finding the pair \((\phi_t, \tau_t)\) such that the Euler method applied to the transformed ODE, \(Z_t' = \phi_t(Z_{\tau_t})\), is as accurate as possible.</p> <h2 id="implications-on-loss-functions">Implications on Loss Functions</h2> <p>Suppose we have a parametric model \(v_t(x;\theta)\) trained to approximate the RF velocity field \(v_t(x)\) under a specific affine interpolation. After training, we can use the relationships derived above to convert this model into \(v'_t(x;\theta)\), corresponding to a different affine interpolation, without retraining. Some questions arise:</p> <p><em>How does training with one interpolation differ from converting an RF trained with another interpolation?</em> <em>Does post-training conversion between models degrade performance?</em></p> <p>It turns out that choosing a different affine interpolation during training is equivalent to changing the <strong>time-weighting</strong> in the loss function and applying an affine transform to the model parameterization. As long as the transformations \(\omega_t\) and \(\tau_t\) are not highly singular, converting a model from one affine interpolation to another may not impact the performance dramatically.</p> <p>Consider a model \(v_t(x; \theta)\) trained to approximate the RF velocity \(v_t\) of interpolation \(X_t = \alpha_t X_1 + \beta_t X_0\) by minimizing a time-weighted mean square loss:</p> \[\mathcal L(\theta) = \int_0^1 \mathbb E\left[ \eta_t \left \| \dot X_t - v_t(X_t;\theta)\right\|^2 \right] \mathrm dt,\tag{4}\] <p>where \(\eta_t\) is a positive time-weighting function.</p> <p>After training, we can convert this model \(v_t(x; \theta)\) into an approximation \(v'_t(x; \theta)\) of \(v'_t\) of a different interpolation \(X'_t = \alpha_t' X_1 + \beta_t' X_0\) via:</p> \[v'_t(x; \theta) = \frac{\dot{\tau}_t}{\omega_t} v_{\tau_t}(\omega_t x; \theta) - \frac{\dot{\omega}_t}{\omega_t} x.\] <p>On the other hand, if we were to train \(v'_t(x; \theta)\) directly to approximate the velocity \(v'_t\) of interpolation \(X'_t = \alpha'_t X_1 + \beta'_t X_0\), the loss function is:</p> \[\mathcal L'(\theta) = \int_0^1 \mathbb{E} \left[ \eta'_t \left\| \dot{X}'_t - v'_t(X'_t; \theta) \right\|^2 \right] \mathrm dt \tag{5}\] <p>Matching the loss \((4)\) and \((5)\), shows that the two training schemes differ only in time-weighting and parameterization. Specifically,</p> \[\eta'_t = \frac{\omega_t^2}{\dot{\tau}_t} \eta_{\tau_t}, \quad v'_t(x; \theta) = \frac{\dot{\tau}_t}{\omega_t} v_{\tau_t}(\omega_t x; \theta) - \frac{\dot{\omega}_t}{\omega_t} x. \tag{6}\] <p>In other words, <strong>training under different affine interpolation schemes is equivalent to applying a different time-weighting function and a corresponding model reparameterization.</strong></p> <blockquote class="example"> <p><strong>Example 2. Loss from Straight to Affine</strong></p> <p>Consider the straight interpolation \(X_t = t X_1 + (1 - t) X_0\) and another affine interpolation \(X_t' = \alpha_t' X_1 + \beta_t' X_0.\)</p> <p>Suppose we have trained a model \(v_t(x, \theta)\) for the straight interpolation with time weights \(\eta_t.\) Then converted \(v_t'(x, \theta)\) corresponds to the RF trained with the parametrization in \((6)\), and a different time-weighting:</p> \[\eta_t' = \frac{\omega_t^2}{\tau_t'} \eta_{\tau_t} = \frac{1}{\dot{\alpha}_t' \beta_t' - \alpha_t' \dot{\beta}_t'} \eta_{\tau_t}.\] <p>Here, we substitute the relationships derived in Example 1 into \((6)\).</p> </blockquote> <h3 id="straight-vs-spherical-identical-training-loss-with-uniform-weights">Straight vs. Spherical: Identical Training Loss With Uniform Weights</h3> <p>Following Example 2, an interesting case arises when \(\dot{\alpha}'_t \beta'_t - \alpha'_t \dot{\beta}'_t\) is constant, in which case \(\eta'_t\) is proportional to \(\eta_{\tau_t}\). Furthermore, if \(\eta_t = 1\) is uniform, then \(\eta'_t\) is also uniform, which implies the two interpolation schemes share the <em>same</em> loss function in this case.</p> <blockquote class="example"> <p><strong>Example 3. Losses for Straight vs. Spherical Interpolation</strong></p> <p>Consider the spherical interpolation: \(X'_t = \sin\left(\frac{\pi t}{2}\right)X_1 + \cos\left(\frac{\pi t}{2}\right)X_0.\)</p> <p>For this choice, \(\dot{\alpha}'_t \beta'_t - \alpha'_t \dot{\beta}'_t = \frac{\pi}{2}\). Hence:</p> \[\eta'_t = \frac{2}{\pi}\eta_{\tau_t}, \quad \tau_t = \frac{\tan\left(\frac{\pi t}{2}\right)}{\tan\left(\frac{\pi t}{2}\right)+1}.\] <p>Thus, training \(v_t(x,\theta)\) for the straight interpolation with a uniform weight (\(\eta_t=1\)) is equivalent to training \(v'_t(x, \theta)\) for the spherical interpolation also with a uniform weight (\(\eta'_t=2/\pi\)). In this case, the only difference in training for these two interpolations is the reparameterization of the model:</p> \[v'_t(x, \theta) = \frac{\pi \omega_t}{2} \left( v_{\tau_t}(\omega_t x; \theta) + \left( \cos\left(\frac{\pi t}{2}\right) - \sin\left(\frac{\pi t}{2}\right) \right) x \right),\] <p>where \(\omega_t = (\sin(\frac{\pi t}{2}) + \cos(\frac{\pi t}{2}))^{-1}\) is bounded within \([1/\sqrt{2}, 1]\).</p> <p>This reparameterization is quite ‚Äúminor‚Äù and does not seem to impact model quality significantly. As shown <a href="#figure-5">Figure 5</a> below, training with straight or spherical interpolation and uniform loss weighting produces nearly identical results.</p> </blockquote> <div class="l-body"> <figure id="figure-6" style="margin: 0em auto;"> <iframe src="/assets/plotly/interp_convert_double_rf.html" frameborder="0" scrolling="no" height="430px" width="75%"> </iframe> <figcaption> <a href="#figure-6">Figure 6</a>. Training the RF with straight and spherical interpolations using uniform weights yields similar results. The blue curve represents the RF trained with spherical interpolation, while the red curve represents the RF trained with straight interpolation and then converted to spherical interpolation. Since both share the same loss function, as shown in Example 3, the only difference lies in model parameterization, which appears to have limited impact on performance in this case. </figcaption> </figure> </div>]]></content><author><name>Rectified Flow Group</name></author><category term="tutorial"/><summary type="html"><![CDATA[Various interpolation schemes have been suggested in different methods. How do they impact performance? Is the simplest straight-line interpolation enough?]]></summary></entry><entry><title type="html">Flow to Diffusion: Langevin is a Guardrail</title><link href="https://rectifiedflow.github.io/blog/2024/diffusion/" rel="alternate" type="text/html" title="Flow to Diffusion: Langevin is a Guardrail"/><published>2024-12-07T10:00:00+00:00</published><updated>2024-12-07T10:00:00+00:00</updated><id>https://rectifiedflow.github.io/blog/2024/diffusion</id><content type="html" xml:base="https://rectifiedflow.github.io/blog/2024/diffusion/"><![CDATA[ <h2 id="overview">Overview</h2> <p>Rectified flow (RF) provides a deterministic ODE (a.k.a. flow) model, of the form \(\mathrm d Z_t = v_t(Z_t)\mathrm d t\), which generates the data \(Z_1\) starting from initial noise \(Z_0\). This approach simplifies the generative process compared to diffusion models, which rely on a stochastic differential equation (SDE) to generate data from noise, such as DDPM and score-based models.</p> <p>Nonetheless, since the work of DDIM<d-cite key="song2020denoising"></d-cite> and probability-flow ODEs<d-cite key="song2020score"></d-cite>, the boundary between flow and diffusion models has been known as blurry. These prior works showed that an SDE can be converted into an ODE during inference without retraining the model. Conversely, starting from ODE models, it is also possible to revert the process and convert the RF ODE into SDEs to obtain stochastic samplers at inference time. This raises several questions:</p> <ol> <li> <p><em>Why and how is it possible to convert between SDEs and ODEs? What is the intuition?</em></p> </li> <li> <p><em>Why would we bother to add diffusion noise given that ODEs are simpler and faster? What are the pros and cons of diffusion vs. flow?</em></p> </li> </ol> <p>This blog post explores these questions. For a more detailed discussion, see Chapter 5 of the <a href="https://www.cs.utexas.edu/~lqiang/PDF/flow_book.pdf">Rectified Flow Lecture Notes</a>. Related works include DDIM<d-cite key="song2020denoising"></d-cite>, score-based SDEs<d-cite key="song2020score"></d-cite>, EDM<d-cite key="karras2022elucidating"></d-cite>.</p> <h2 id="stochastic-samplers--rf--langevin">Stochastic Samplers = RF + Langevin</h2> <p>Given a coupling \((X_0, X_1)\) of noise and data points, rectified flow defines an interpolation process, such as \(X_t = t X_1 + (1 - t) X_0\), and ‚Äúrectifies‚Äù or ‚Äúcausalizes‚Äù it to yield an ODE model \(\mathrm{d} Z_t = v_t(Z_t) \, \mathrm{d} t\) initialized from \(Z_0 = X_0.\) The velocity field is given by \(v_t(z) = \mathbb{E}[\dot{X}_t \mid X_t = z],\) which is estimated by minimizing a loss like \(\mathbb{E}_{t, X_0, X_1} [ \| \dot{X}_t - v_t(X_t) \|^2 ].\)</p> <p>A key property of the RF ODE is its <em>marginal preserving property</em>: the distribution of \(Z_t\) on the ODE trajectory matches the distribution of \(X_t\) on the interpolation path at each time \(t\). This is ensured by constructing the velocity field \(v_t\) in an inductive way: if the distributions of \(X_t\) and \(Z_t\) match up to a given time, the constructed \(v_t\) guarantees they will continue to match at the next infinitesimal step. As a result, the final output \(Z_1\) of the ODE follows the same distribution as \(X_1\), the target data distribution. By being ‚Äúscheduled to do the right thing at the right time,‚Äù the process guarantees the correct final result.</p> <p>However, one challenge is that errors can accumulate over time as we solve the ODE \(\mathrm{d} Z_t = v_t(Z_t) \mathrm{d} t\) in practice. These errors arise from both model approximations and numerical discretization, causing drift between the estimated distribution and the true distribution. The problem can compound: if the estimated trajectory \(\hat{Z}_t\) deviates significantly from the distribution of \(X_t\), the update direction \(v_t(\hat{Z}_t)\) becomes less accurate and can further reinforce the deviation.</p> <p>To address this problem, we may introduce a <strong>feedback mechanism</strong> to correct the errors. One such approach is to use Langevin dynamics.</p> <blockquote class="definition"> <p><strong>Langevin Dynamics.</strong> For a density function \(\rho^*(x)\), its (discrete-time) Langevin dyamics is</p> \[\hat{Z}_{t+\epsilon} = \hat{Z}_t + \epsilon \sigma_t^2 \nabla \log \rho^*(\hat{Z}_t) + \sqrt{2\epsilon}\,\sigma_t\,\xi_t,\quad \xi_t \sim \mathtt{Normal}(0, I),\] <p>where \(\sigma_t\) is the diffusion coefficient, and \(\epsilon&gt;0\) is the step size. Intuitively, this update is gradient ascent on log probability \(\log \rho^*\) with Gaussian noise perturbations of variance \(\epsilon\) at each step. Langevin dynamics provides an approximate method to draw samples from \(\rho^*\) because, under regularity conditions, the distribution of \(Z_t\) converges to \(\rho^*\) as \(t \to +\infty\) and \(\epsilon \to 0\).</p> <p>When the step size \(\epsilon\) approaches to zero, the continuous-time limit of this update is written as a stochastic differential equation (SDE):</p> \[\mathrm{d} Z_t = \sigma_t^2 \nabla \log \rho^*(Z_t) \mathrm{d} t + \sqrt{2}\sigma_t \mathrm{d} W_t,\] <p>where \(\{W_t\}\) is a Brownian motion, which has independent increments (for every \(t &gt; 0\), the future increments \(\{W_{t+u} - W_t,\, u \ge 0\}\) are independent of the past trajectory \(\{W_s,\, s &lt; t\}\)) and Gaussian increments \(\bigl(W_{t+u} - W_t \sim \mathcal{N}(0, u)\bigr)\).</p> <p>We do not need to delve deeply into SDE theory here. It suffices to substitute the SDE with the discrete-time update, understanding that the SDE represents the continuous-time limit of the discrete-time update in a suitable sense, which mathematicians have already clarified.</p> </blockquote> <p>Let \(\rho_t\) be the density function of \(X_t\), representing the true distribution that we aim to follow at time \(t\). At each time step \(t\), we can in principle apply a short segment of Langevin dynamics to adjust the trajectory‚Äôs distribution toward \(\rho_t\):</p> \[\mathrm{d} Z_{t, \tau} = \sigma_t^2 \nabla \log \rho_t(Z_{t, \tau}) \, \mathrm{d} \tau + \sqrt{2} \, \sigma_t \, \mathrm{d} W_\tau, \quad \tau \geq 0,\] <p>where \(\tau\) is an auxiliary time scale for the Langevin dynamics. This yields a double-loop algorithm in which the system is simulated to equilibrium \((\tau \to \infty)\) at each \(t\) before moving on to the next time point.</p> <p>In rectified flow, however, the trajectory is already close to \(\rho_t\) at each time step \(t\). Therefore, a single step of Langevin dynamics can be sufficient to mitigate the drift. This allows us to directly integrate Langevin corrections into the rectified flow updates, yielding a combined SDE:</p> \[\mathrm{d}{Z}_t = \underbrace{v_t({Z}_t) \,\mathrm{d} t}_{\textcolor{blue}{\text{Rectified Flow}}} + \underbrace{\sigma_t^2 \nabla \log \rho_t({Z}_t)\,\mathrm{d} t + \sqrt{2}\,\sigma_t\,\mathrm{d}W_t}_{\textcolor{red}{\text{Langevin Dynamics}}} ,\quad \tilde{Z}_0 = Z_0.\] <p>This combined SDE achieves two primary objectives:</p> <div class="l-body"> <figure id="figure-0" style="margin: 0em auto;"> <div style="display: flex; justify-content: center;"> <img src="/assets/img/sde_velocity.png" alt="velocity" style="width: 25%; height: auto; margin-right: 5em;"/> <img src="/assets/img/sde_score.png" alt="score funtion" style="width: 25%; height: auto;"/> </div> </figure> </div> <ol> <li> <p>The <strong><span style="color:blue;">rectified flow</span></strong> drives the generative process forward as intended.</p> </li> <li> <p>The <strong><span style="color:red;">Langevin component</span></strong> acts as a negative feedback loop, correcting distributional drift without bias when \(\tilde{Z}_t\) and \(\rho_t\) are well aligned.</p> </li> </ol> <p>When the simulation is accurate, Langevin dynamics naturally remain in equilibrium, avoiding unnecessary changes to the distribution. However, if deviations occur, this mechanism guides the estimate back on track, enhancing the robustness of the inference.</p> <div class="l-body"> <figure id="figure-1" style="margin: 1em auto;"> <div style="display: flex; justify-content: center;"> <iframe src="/assets/plotly/diffusion_score_function.html" frameborder="0" scrolling="no" height="450px" width="80%"></iframe> </div> <figcaption> <a href="#figure-1">Figure 1</a>. Illustration of the (normalized) score function \(\nabla \log \rho_t\) along the SDE trajectories. We can see that \(\nabla \log \rho_t\) points toward high-density regions, helping guide trajectories back to areas of higher probability whenever deviations occur. </figcaption> </figure> </div> <div class="l-body"> <figure id="figure-2" style="margin: 1em auto;"> <div style="display: flex;"> <iframe src="/assets/plotly/diffusion_deterministic_single.html" frameborder="0" scrolling="no" height="300px" width="45%"></iframe> <iframe src="/assets/plotly/diffusion_stochastic_single.html" frameborder="0" scrolling="no" height="300px" width="45%"></iframe> </div> <figcaption> <a href="#figure-2">Figure 2</a>. Comparing two sampling methods. On the left, we apply Euler discretization to the deterministic ODE using a "poor" \(v_t\) (due to early stopping), resulting in many outliers. On the right, the Euler‚ÄìMaruyama method simulates the SDE, effectively suppressing outliers through the feedback of the score function. </figcaption> </figure> </div> <p>This correction mechanism also has an effect on state-of-the-art text-to-image generation. In <a href="https://arxiv.org/abs/2411.19415">a recent work</a><d-cite key="hu2024amo"></d-cite>, we found that stochastic samplers improve text rendering quality over deterministic samplers in SOTA models such as Flux. Moreover, they produce images that better reflect the text prompt. The figure below shows that using a stochastic sampler on the Flux model consistently outperforms the deterministic Euler sampler in text rendering quality across all step sizes. The right side shows qualitative examples demonstrating the enhanced text rendering achieved by the stochastic sampler.</p> <div class="l-body"> <img src="/assets/img/flux_text_rendering.png" alt="flux_text_rendering" style="max-width:100%; margin-bottom: 20px"/> </div> <h2 id="sdes-with-tweedies-formula">SDEs with Tweedie‚Äôs formula</h2> <p>Solving the SDEs requires estimating the score function \(\nabla \log \rho_t\) in addition to the RF velocity \(v_t\). However, in certain special cases, the score function can be estimated from \(v_t\), thus avoiding the need to retrain an additional model. This enables a training-free conversion between ODEs and SDEs.</p> <p>Specifically, if the rectified flow is induced by an affine interpolation \(X_t = \alpha_t X_1 + \beta_t X_0\), where \(X_0\) and \(X_1\) are independent (i.e., \(X_0 \perp\!\!\!\perp X_1\)) and \(X_0\) follows a standard Gaussian distribution, then by <a href="https://en.wikipedia.org/wiki/Maurice_Tweedie#Tweedie's_formula">Tweedie‚Äôs formula</a>, we have</p> \[\nabla \log \rho_t(x) = -\frac{1}{\beta_t} \mathbb{E}[X_0 \mid X_t = x].\] <p>You don‚Äôt need to know the exact proof for Tweedie‚Äôs formula to apply it; focusing on its application is sufficient. However, for those interested in the mathematics, please refer to the following proof:</p> <details> <summary>Proof of Tweedie's fomular (Click to expand)</summary> Theorem 1: For any pair of random variables (X, Z), we have \begin{equation} \nabla_{x}logP_{X}(x) = \mathbb{E}[\nabla_{x} logP_{X, Z}(x,z) | X] \end{equation} Proof: \begin{align} LHS &amp;= \nabla_{x}logP_{X}(x) \nonumber\\ &amp;= \frac{\nabla_{x}P_{X}(x)}{P_{X}(x)} \nonumber \\ &amp;= \frac{\nabla_{x}\int P_{X, Z}(x,z)dz}{P_{X}(x)} \nonumber \\ &amp;= \frac{\int \nabla_{x}P_{X, Z}(x,z)dz}{P_{X}(x)} \nonumber \\ &amp;= \int \frac{\nabla_{x}P_{X, Z}(x,z)}{P_{X,Z}(x,z)} \cdot \frac{P_{X,Z}(x, z)}{P_{X}(x)}dz \nonumber \\ &amp;= \int \nabla_{x}logP_{X,Z}(x,z)\cdot \frac{P_{X,Z}(x, z)}{P_{X}(x)}dz \nonumber \\ &amp;=\mathbb{E}[\nabla_{x} logP_{X,Z}(x,z) | X] = RHS \nonumber \end{align} Theorem 2: If X = Y + Z, Y $\perp$ Z (Y and Z are independent), then \begin{equation} \nabla_{x}logP_{X}(x) = \mathbb{E}[\nabla_{z} logP_{Z}(z) | X] = \mathbb{E}[\nabla_{y} logP_{Y}(y) | X] \end{equation} Proof: \begin{align} LHS &amp;= \nabla_{x}logP_{X}(x) \nonumber \\ &amp;= \frac{\nabla_{x}P_{X}(x)}{P_{X}(x)} \nonumber \\ &amp;=\frac{\nabla_{x}\int P_{X, Z}(x,z)dz}{P_{X}(x)} \nonumber \\ &amp;= \frac{\nabla_{x}\int P_{Z}(z)P_{Y}(x-z)dz}{P_{X}(x)} \nonumber \\ &amp; (\text{Since } P_{X,Z}(x,z) = P_{Z}(z)P_{X}(x|Z=z) = P_{Z}(z)P_{Y}(x-z)) \nonumber\\ &amp;= \frac{\int \nabla_{x}P_{Z}(z)P_{Y}(x-z)dz}{P_{X}(x)} \nonumber \\ &amp;= \frac{\int P_{Z}(z) \nabla_{x}P_{Y}(x-z)dz}{P_{X}(x)} \nonumber \\ &amp;(\text{This is because } P_{Z}(z) \text{is independent of } x) \nonumber\\ &amp;= \int \frac{\nabla_{x}P_{Y}(x-z)}{P_{Y}(x-z)} \cdot \frac{P_{Z}(z)P_{Y}(x-z)}{P_{X}(x)} dz \nonumber \\ &amp;= \int \nabla_{x}logP_{Y}(x-z) \cdot \frac{P_{Z}(z)P_{Y}(x-z)}{P_{X}(x)} dz \nonumber \\ &amp;= \mathbb{E}[\nabla_{x}logP_{Y}(x-z)|X] \nonumber \\ &amp;= \mathbb{E}[\nabla_{y}logP_{Y}(y)|X] = RHS \nonumber \end{align} Theorem 3: If X = Y + Z, Y $\perp$ Z, and Z $\sim \mathcal{N}(0, \sigma^{2}I)$, then \begin{equation} \nabla_{x}logP_{X}(x) = -\frac{1}{\sigma^{2}}\mathbb{E}[Z|X] = \frac{1}{\sigma^{2}}(\mathbb{E}[Y|X] - X) \end{equation} Proof: \begin{align} Z \sim N(0, \sigma^{2}I) &amp;\implies P_{Z}(z) \propto exp(-\frac{z^{2}}{2\sigma^{2}}) \implies \nabla_{z}logP_{Z}(z) = -\frac{z}{\sigma^{2}} \nonumber \\ \nabla_{x}logP_{X}(x) &amp;= \mathbb{E}[\nabla_{z} logP_{Z}(z) | X] \nonumber \\ &amp; \text{(Applying Theorem 2)} \nonumber\\ &amp;= -\frac{1}{\sigma^{2}} \mathbb{E}[Z|X] \nonumber \\ &amp;= -\frac{1}{\sigma^{2}} \mathbb{E}[X-Y|X] \nonumber \\ &amp;= \frac{1}{\sigma^{2}}(\mathbb{E}[Y|X] - X) \nonumber \end{align} In fact, we could let $X = X_t$, $Y = \alpha_t X_1$ and $Z = \beta_t X_0$. We know that $Z \sim \mathcal{N}(0, \beta_t^{2}I)$. Using Theorem 3, we get \begin{align} \nabla \log \rho_t(x) &amp;= \frac{1}{\beta_t^2}(\mathbb{E}[\alpha_t X_1 | X_t=x] - x) \nonumber \\ &amp;= \frac{1}{\beta_t^2}(\mathbb{E}[\alpha_t X_1 - x | X_t=x]) \nonumber \\ &amp;= \frac{1}{\beta_t^2}(\mathbb{E}[\beta_t X_0 | X_t=x]) \nonumber \\ &amp;= \frac{1}{\beta_t}(\mathbb{E}[X_0 | X_t=x]) \nonumber \\ \end{align} </details> <p>On the other hand, the RF velocity is given by</p> \[v_t(x) = \mathbb{E}[\dot{X}_t \mid X_t = x] = \mathbb{E}[\dot{\alpha}_t X_1 + \dot{\beta}_t X_0 \mid X_t = x].\] <p>Using this, we can express \(\mathbb{E}[X_0 \mid X_t = x]\) in terms of \(v_t(x)\) and obtain</p> \[\nabla \log \rho_t(x) = \frac{\alpha_t v_t(x) - \dot{\alpha}_tx}{\lambda_t\beta_t}, \quad \text{where } \lambda_t = \dot{\alpha}_t \beta_t - \alpha_t \dot{\beta}_t.\] <p>As a result, the SDE takes the form</p> \[\mathrm d Z_t = v_t(Z_t)\mathrm d t + \gamma (\alpha_t v_t (Z_t) - \dot \alpha_t Z_t) \mathrm{d} t + \sqrt{2 \lambda_t \beta_t \gamma_t} \mathrm{d} W_t,\] <p>where we set \(\sigma_t^2 = \lambda_t \beta_t \gamma_t\).</p> <p>In the case of straight interpolation \(X_t = t X_1 + (1-t)X_0\), we have \(\nabla \log \rho_t(x) = \frac{t v_t(x) - x}{1-t}\), leading to</p> \[\mathrm d Z_t = v_t(Z_t)\mathrm d t + \gamma_t (t v_t (x) - x) \mathrm{d} t + \sqrt{2 \gamma_t (1-t) } \mathrm{d} W_t.\] <p>The SDE of DDPM and the score-based SDEs can be recovered by setting \(\gamma_t = 1 / \alpha_t\) and \(\alpha^2_t + \beta_t^2 = 1\), giving</p> \[\mathrm{d} Z_t = 2 v_t(Z_t) \, \mathrm{d} t - \frac{\dot{\alpha}_t}{\alpha_t} Z_t \, \mathrm{d} t + \sqrt{2 \frac{\dot \alpha_t}{\alpha_t}} \mathrm{d} W_t.\] <h2 id="diffusion-may-cause-over-concentration">Diffusion May Cause Over-Concentration</h2> <p>Although things work out nicely in theory, we need to be careful that the introduced score function \(\nabla \log \rho_t(x)\) itself has errors, and it may introduce undesirable effects if we rely on it too much (by using a large \(\sigma_t\)). This is indeed the case in practice. As shown in the figure below, when we increase the noise magnitude \(\sigma_t\), the generated samples tend to cluster closer to the centers of the Gaussian modes.</p> <div class="l-body"> <figure id="figure-4" style="margin: 1em auto;"> <div style="display: flex; justify-content: center;"> <iframe src="/assets/plotly/diffusion_noise_scales_4pics.html" frameborder="0" scrolling="no" height="220px" width="100%"></iframe> </div> <figcaption> <a href="#figure-4"></a> </figcaption> </figure> </div> <p>So, larger diffusion yields more concentrated results? This appears counterintuitive at first glance. Why does this happen?</p> <p>To see this, assume the estimated velocity field is \(\hat v_t \approx v_t\). The corresponding estimated score function from Tweedie‚Äôs formula becomes</p> \[\nabla \log \hat \rho_t(x) = \frac{1}{\lambda_t \beta_t} \left( \alpha_t \hat v_t(x) - \dot{\alpha}_t x \right).\] <p>Because \(\beta_t\) must converge to 0 as \(t \to 1\), the estimated score function \(\nabla \log \hat{\rho}_t(x)\) would diverge to infinity in this limit. On the other hand, the true magnitude of \(\nabla \log \rho_t(x)\) may be finite, thus being significantly overestimated when \(t\) is close to 1. Since \(\nabla \log \rho_t(x)\) point toward the centers of mass of clusters, its overestimation leads to an overly concentrated distribution around these centers.</p> <blockquote class="theorem"> <p><strong>Role of Noise.</strong> In summary, the Langevin guardrail can become too <em>excessive</em>, causing over-concentration. It is the score function \(\nabla \log \rho_t(x)\) that drives this concentration, rather than the noise itself, as one might initially assume from the ODE vs. SDE dichotomy. The noise component in Langevin dynamics compensates for the concentration induced by the score function, but it does not necessarily prevent it when the score is overestimated.</p> </blockquote> <div class="l-body"> <figure id="figure-3" style="margin: 1em auto;"> <div style="display: flex; justify-content: center;"> <img src="/assets/img/sde_turn_off_noise.png" alt="velocity" style="width: 25%; height: auto; margin-right: 5em;"/> </div> <figcaption> <a href="#figure-3">Figure 3</a>. If we remove the Langevin noise but keeps the score function, i.e., simulating the ODE \(\mathrm{d} Z_t = v_t(Z_t) \mathrm{d} t + \sigma_t^2 \nabla \log \rho_t(Z_t) \mathrm{d} t\), the dynamics would collapse to local modes of the distribution. </figcaption> </figure> </div> <p>In the context of text-to-image generation, this over-concentration effect often produces overly smoothed images, which sometimes appear cartoonish. Such over-smoothing eliminates fine details and high-frequency variations, resulting in outputs with a blurred appearance. The figure below illustrates these differences: samples generated using the Euler sampler exhibit more high-frequency details, as seen in the texture of the parrot‚Äôs feathers and the structure of the smoke.</p> <div class="l-body"> <img src="/assets/img/euler_sde_comp_flux.png" alt="cross" style="max-width:100%;margin-bottom: 20px"/> </div>]]></content><author><name>Xixi Hu</name></author><category term="tutorial"/><summary type="html"><![CDATA[It is known that we can convert between diffusion (SDE) and flow (ODE) models at inference time without retraining. But how is this possible? What is the intuition and purpose? What are the pros and cons of diffusion vs. flow?]]></summary></entry><entry><title type="html">Rectified Flow: Straight is Fast</title><link href="https://rectifiedflow.github.io/blog/2024/intro/" rel="alternate" type="text/html" title="Rectified Flow: Straight is Fast"/><published>2024-12-06T10:00:00+00:00</published><updated>2024-12-06T10:00:00+00:00</updated><id>https://rectifiedflow.github.io/blog/2024/intro</id><content type="html" xml:base="https://rectifiedflow.github.io/blog/2024/intro/"><![CDATA[ <h2 id="overview">Overview</h2> <p>This blog provides a brief introduction to rectified flow, based on Chapter 1 of these <a href="https://www.cs.utexas.edu/~lqiang/PDF/flow_book.pdf">lecture notes</a>. For more introduction, please refer to the original papers<d-cite key="liu2022flow,liu2022rectified"></d-cite> and these <a href="https://www.cs.utexas.edu/~lqiang/rectflow/html/intro.html">blogs</a>.</p> <h2 id="problem-learning-flow-generative-models">Problem: Learning Flow Generative Models</h2> <p>Generative modeling can be formulated as finding a computational procedure that transforms a noise distribution, denoted by \(\pi_0\), into an unknown data distribution \(\pi_1\) observed from data. In flow models, this procedure is represented by an ordinary differential equation (ODE):</p> \[\dot{Z}_t = v_t(Z_t), \quad \forall t \in [0,1], \quad \text{starting from } Z_0 \sim \pi_0, \tag{1}\] <p>where \(\dot{Z}_t = \mathrm dZ_t / \mathrm dt\) denotes the time derivative, and the velocity field \(v_t(x) = v(x, t)\) is a learnable function to be estimated to ensure that \(Z_1\) follows the target distribution \(\pi_1\) when starting from \(Z_0 \sim \pi_0\). In this case, we say that the stochastic process \(Z = \{Z_t\}\) provides an (ODE) transport from \(\pi_0\) to \(\pi_1\).</p> <p>It is important to note that, in all but trivial cases, there exist <em>infinitely many</em> ODE transports from \(\pi_0\) to \(\pi_1\), provided that at least one such process exists. Thus, it is essential to be clear about which types of ODEs we should prefer.</p> <p>One option is to favor ODEs that are <strong><em>easy</em></strong> to solve at inference time. In practice, the ODEs are approximated using numerical methods, which typically construct <em>piecewise linear</em> approximations of the ODE trajectories. For instance, a common choice is the Euler method:</p> \[\hat{Z}_{t+\epsilon} = \hat{Z}_t + \epsilon v_t(\hat{Z}_t), \quad \forall t \in \{0, \epsilon, 2\epsilon, \dots, 1\}, \tag{2}\] <p>where \(\epsilon &gt; 0\) is a step size. Varying the step size \(\epsilon\) introduces a trade-off between accuracy and computational cost: smaller \(\epsilon\) yields higher accuracy but requires more computation steps. Therefore, we should seek ODEs that can be approximated accurately even with large step sizes.</p> <figure id="figure-1" style="margin: 1em auto;"> <div style="display: flex; justify-content: center;"> <iframe src="/assets/plotly/intro_euler_method.html" frameborder="0" scrolling="no" height="300px" width="100%" style="max-width: 400px;"> </iframe> </div> <figcaption> <a href="#figure-1">Figure 1</a>. Lady Windermere's fan for illustration of error accumulation in Euler method trajectories, starting from various initial points and deviating from the true solution curve over time. </figcaption> </figure> <p>The ideal scenario arises when the ODE follows straight-line trajectories, in which case Euler approximation yields <em>zero discretization error</em> regardless of the choice of step sizes. In such cases, the ODE, up to time reparameterization, should satisfy:</p> \[Z_t = t Z_1 + (1 - t) Z_0, \quad \implies \quad \dot{Z}_t = Z_1 - Z_0.\] <p>These ODEs, known as <em>straight transports</em>, enable <em>fast</em> generative models that can be simulated in a single step. We refer to the resulting pair \((Z_0, Z_1)\) as a straight coupling of \(\pi_0\) and \(\pi_1\). In practice, we may not achieve perfect straightness, but we can aim to make the ODE trajectories as straight as possible to maximize computational efficiency.</p> <h2 id="rectified-flow">Rectified Flow</h2> <p>To construct a flow transporting \(\pi_0\) to \(\pi_1\), let us assume that we are given an arbitrary coupling \((X_0, X_1)\) of \(\pi_0\) and \(\pi_1\), from which we can obtain empirical draws. This can simply be the <em>independent coupling</em> with law \(\pi_0 \times \pi_1\), as is common in practice when we have access to independent samples from \(\pi_0\) and \(\pi_1\). The idea is to take \((X_0, X_1)\) and convert it to a better coupling generated by an ODE model. Optionally,¬†we¬†can¬†then¬†iteratively¬†repeat¬†this¬†process¬†to¬†further¬†enhance¬†desired¬†properties,¬†such¬†as¬†straightness.</p> <p>Rectified flow is constructed in the following ways:</p> <ul> <li><strong>Build Interpolation:</strong></li> </ul> <p>The first step is to build an interpolation process \(\{X_t\} = \{X_t : t \in [0, 1]\}\) that smoothly interpolates between \(X_0\) and \(X_1\)‚Äã. Although general choices are possible, let us consider the canonical choice of straight-line interpolation:</p> \[X_t = t X_1 + (1 - t) X_0.\] <p>Here the interpolation \(\{X_t\}\) is a stochastic process generated in an <strong>‚Äúanchor-and-bridge‚Äù</strong> way: we first sample the endpoints \(X_0\) and \(X_1\) and then sample the intermediate trajectory connecting them.</p> <ul> <li><strong>Marginal Matching:</strong></li> </ul> <p>By construction, the marginal distributions of \(X_0\) and \(X_1\) match the target distributions \(\pi_0\) and \(\pi_1\) through the interpolation process \(\{X_t\}\). However, \(\{X_t\}\) is not a <em>causal</em> ODE process like \(\dot{Z}_t = v_t(Z_t)\), which generate the output \(Z_1\) by evolving forward in time from \(Z_0\). Instead, generating \(X_t\) requires knowledge of both \(X_0\) and \(X_1\), rather than evolving solely from \(X_0\) as \(t\) increases.</p> <p>This issue can be resolved if we can convert \(\{X_t\}\) somehow into a causal ODE process while preserving the marginal distributions of \(X_t\) at each time \(t\). Note that since we only care about the output \(X_1\), we only need to match the marginal distributions of \(X_t\) at each individual time \(t\). There is no need to match the trajectory-wise joint distribution of \(\{X_t\}\).</p> <p>Perhaps surprisingly, marginal matching can be achieved by simply training the velocity field \(v_t\) of the ODE model \(\dot{Z}_t = v_t(Z_t)\) to match the slope \(\dot{X}_t\) of the interpolation process via:</p> \[\min_v \int_0^1 \mathbb{E} \left[ \left\| \dot{X}_t - v_t(X_t) \right\|^2 \right] \mathrm dt. \tag{3}\] <p>The theoretical minimum is achieved by:</p> \[v_t^*(x) = \mathbb{E} \left[ \dot{X}_t \mid X_t = x \right],\] <p>which is the conditional expectation of the slope \(\dot{X}_t\) for all the interpolation trajectories passing through a given point \(X_t = x\). If multiple trajectories pass point \(X_t=x\), the velocity \(v_t^*(x)\) is the average of \(\dot X_t\) for these trajectories.</p> <p>With the canonical straight interpolation \(X_t = t X_1+(1-t)X_0\), we have \(\dot{X}_t = X_1 - X_0\) by taking the derivative of \(X_t\) with respect to \(t\). It yields:</p> \[\min_v \int_0^1 \mathbb{E} \left[ \| \dot{X}_t - v_t(X_t) \|^2 \right] \mathrm dt, \quad X_t = t X_1 + (1 - t) X_0.\] <p>In practice, the optimization in (3) can be efficiently solved even for large AI models when \(v\) is parameterized as modern deep neural nets. This is achieved by leveraging off-the-shelf optimizers with stochastic gradients, computed by drawing pairs \((X_0, X_1)\) from data, sampling \(t\) uniformly in \([0, 1]\), and then computing the corresponding \((X_t, \dot{X}_t)\) using the interpolation formula.</p> <blockquote class="theorem"> <p><strong>Notation.</strong> A stochastic process \(X_t = X(t, \omega)\) is a measurable function of time \(t\) and a random seed \(\omega\) (with, say, distribution \(\mathbb{P}\)). In the case above, the end points are the random seed, i.e., \(\omega = (X_0, X_1)\). The slope is given by \(\dot{X}_t = \partial_t X(t, \omega)\) as the partial derivative of \(X\) w.r.t. \(t\), which is also a function of the same random seed. The expectation in the loss, written in full, is</p> \[\mathbb{E}_{\omega \sim \mathbb{P}} \left[ \left\| \partial_t X(t, \omega) - v_t(X(t, \omega)) \right\|^2 \right].\] <p>In writing, we often omit the random seed. Whenever we take the expectation, it averages out all random sources inside the brackets except for those explicitly included in the conditioning.</p> </blockquote> <div class="l-body"> <figure id="figure-2" style="margin: 1em auto;"> <iframe src="/assets/plotly/intro_rf_three_in_one.html" frameborder="0" scrolling="no" height="380px" width="105%"> </iframe> <figcaption> <a href="#figure-2">Figure 2</a>. Rectified flow between \(\pi_0\) and \(\pi_1\). Blue and pink lines represent trajectories, colored by the mode they are associated with for visualization. </figcaption> </figure> </div> <p>We illustrate the intuition in Fig.2:</p> <ul> <li>In the interpolation process \(\{X_t\}\), different trajectories may have intersecting points, resulting in multiple possible values of \(\dot X_t\) associated with a same point \(X_t\) due to uncertainty about which trajectory it was drawn from (Fig.2a).</li> <li>In contrast, by the definition of an ODE \(\dot{Z}_t = v_t^*(Z_t)\), the update direction \(\dot{Z}_t\) at each point \(Z_t\) is uniquely determined by \(Z_t\), making it impossible for different trajectories of ${Z_t}$ to intersect and then diverge along different directions.</li> <li>Hence at these intersection points of \(\{X_t\}\) where ${\dot X_t}$ is uncertain and non-unique, the ODE \(\{Z_t\}\) ‚Äúderandomizes‚Äù the update direction by following the conditional expectation \(\displaystyle v_t^*(X_t) = \mathbb{E}[\dot{X}_t \mid X_t].\) Consequently, the trajectories of the ODE ‚Äúreassemble‚Äù the interpolation trajectories in a way that avoids intersections. See Fig.2(b).</li> <li>Since ODE trajectories ${Z_t}$ cannot intersect, they must curve at potential intersection points to ‚Äúrewire‚Äù the original interpolation paths and avoid crossing.</li> </ul> <blockquote class="definition"> <p><strong>Rectified Flow.</strong> For any time-differential stochastic process \(\{X_t\} = \{X_t : t \in [0, 1]\}\), we call the ODE process:</p> \[\dot{Z}_t = v_t^*(Z_t) \quad \text{with} \quad v_t^*(x) = \mathbb{E} \left[ \dot{X}_t \mid X_t = x \right], \quad Z_0 = X_0\] <p>the <strong>rectified flow</strong> induced by \(\{X_t\}\). We denote it as:</p> \[\{Z_t\} = \texttt{Rectify}(\{X_t\}).\] </blockquote> <div class="l-body"> <figure id="figure-3" style="margin: 1em auto;"> <img src="/assets/img/flow_static.png" alt="A close-up view of how rectification rewires interpolation trajectories" width="80%"/> <figcaption> <a href="#figure-3">Figure 3</a>. A close-up view of how rectification ‚Äúrewires‚Äù interpolation trajectories. (a) Interpolation trajectories with intersections. (b) Averaged velocity directions at intersection points (red arrows). (c) Trajectories of the resulting rectified flow. </figcaption> </figure> </div> <p>Figure 3 illustrates a close-up view of how rectification ‚Äúrewires‚Äù interpolation trajectories. Consider two ‚Äúbeams‚Äù of interpolation trajectories intersecting to form the ‚Äúregion of confusion‚Äù (shaded area in the middle). Within this region, a particle moving along the rectified flow follows the averaged direction $v^*_t$. Upon exiting, the particle joins one of the original interpolation streams based on its exit side and continues moving. Since rectified flow trajectories do not intersect within the region, they remain separated and exit from their respective sides, effectively ‚Äúrewiring‚Äù the original interpolation trajectories.</p> <p>What makes rectified flow \(\{Z_t\}\) useful is that it preserves the marginal distributions of \(\{X_t\}\) at each point while resulting in a ‚Äúbetter‚Äù coupling \((Z_0, Z_1)\) in terms of optimal transport:</p> <ol> <li> <p><strong>Marginal Preservation</strong></p> <p>The \(\{X_t\}\) and its rectified flow \(\{Z_t\}\) share the same marginal distributions at each time \(t \in [0, 1]\), that is:</p> \[\text{Law}(Z_t) = \text{Law}(X_t), \quad \forall t \in [0, 1],\] <p>where \(\text{Law}(X_t)\) denotes the probability distribution (or law) of random variable \(X_t\).</p> <p>Intuitively, by the definition of \(v_t\) in (1), the total amount of mass flow entering and exiting every infinitesimal volume in the space is equal under the dynamics of \(X_t\) and \(Z_t\). This ensures that the two processes yield the same marginal distributions, even though the flow directions may differ.</p> <div class="l-body"> <img src="/assets/img/flow_in_out.png" alt="cross" style="max-width:100%;"/> </div> </li> <li> <p><strong>Transport Cost</strong></p> <p>The start-end pairs \((Z_0, Z_1)\) from the rectified flow \(\{Z_t\}\) guarantee to yield no larger transport cost than \((X_0, X_1)\), simultaneously for all convex cost functions \(c\):</p> \[\mathbb{E} \left[ c(Z_1 - Z_0) \right] \leq \mathbb{E} \left[ c(X_1 - X_0) \right], \quad \forall \text{convex } c : \mathbb{R}^d \to \mathbb{R}.\] <p>Intuitively, it is because disentangling the intersections reduces the length of the trajectories by triangle inequality:</p> <div class="l-body"> <img src="/assets/img/flow_transport_cost.png" alt="cost" style="max-width:100%;"/> </div> </li> </ol> <h2 id="reflow">Reflow</h2> <p>While rectified flows tend to favor straight trajectories, they are not perfectly straight. As in <a href="#figure-2">Fig.2 (a)</a>, the flow makes turns at intersection points of the interpolation trajectories \(\{X_t\}\). How can we further improve the flow to achieve straighter trajectories and hence speed up inference?</p> <p>A key insight is that the start-end pairs \((Z_0, Z_1)\) generated by rectified flow, called the <strong>rectified coupling</strong> of \((X_0, X_1)\), form a better and ‚Äústraighter‚Äù coupling compared to \((X_0, X_1)\). This is because if we connect \(Z_0\) and \(Z_1\) with a new straight-line interpolation, it would yield fewer intersection points. Hence, training a new rectified flow based on this interpolation would result in straighter trajectories, leading to faster inference.</p> <p>Formally, we apply the \(\texttt{Rectify}(¬∑)\) procedure recursively, yielding a sequence of rectified flows starting from \((Z_0^0, X_1^0) = (X_0, X_1)\):</p> \[\texttt{Reflow:} \quad \quad \{Z_t^{k+1}\} = \texttt{Rectify}(\texttt{Interp}(Z_0^k, Z_1^k)),\] <p>where \(\text{Interp}(Z_0^k, Z_1^k)\) denotes an interpolation process given \((Z_0^k, Z_1^k)\) as the endpoints. We call \(\{Z_t^k\}\) the \(k\)-th rectified flow, or simply the <strong>\(k\)-rectified flow</strong>, induced from \((X_0, X_1)\).</p> <p>This reflow procedure is proved to ‚Äústraighten‚Äù the paths of rectified flows in the following sense: Define the following measure of straightness of \(\{Z_t\}\):</p> \[S(\{Z_t\}) = \int_0^1 \mathbb{E} \left[ \|Z_1 - Z_0 - \dot{Z}_t\|^2 \right] \mathrm dt,\] <p>where \(S(\{Z_t\})\) is a measure of the straightness of \(\{Z_t\}\), with \(S(\{Z_t\}) = 0\) corresponding to straight paths. Then it can be found in paper<d-cite key="liu2022flow"></d-cite> that</p> \[\mathbb{E}_{k \sim \text{Unif}(\{1, \dots, K\})} \left[S(\{Z_t^k\})\right] = \mathcal{O}(1 / K),\] <p>which suggests that the average of \(S(\{Z_t^k\})\) in the first \(K\) steps decay with an \(\mathcal{O}(1 / K)\) rate.</p> <p>Note that reflow can begin from any coupling \((X_0, X_1)\), so it provides a general procedure for straightening and thus speeding up any given dynamics while preserving the marginals.</p> <p>As shown in <a href="#figure-2">Fig.2 (c)</a>, after applying the ‚ÄúReflow‚Äù operation, the trajectories become straighter than the original rectified flow \(Z_t\).</p> <blockquote class="example"> <p><strong>Reflow and Shortcut Learning.</strong> Intuitively, reflow resembles shortcut learning in humans: once we solve a problem for the first time, we learn to go directly to the solution, enabling us to solve it more quickly the next time.</p> </blockquote>]]></content><author><name>Runlong Liao</name></author><category term="tutorial"/><summary type="html"><![CDATA[Rectified flow learns ODEs as generative models by causalizing (or rectifying) an interpolation process that smoothly connects noise and data. This process naturally favors dynamics with straighter trajectories and hence fast Euler discretization, and can be repeated to further improve straightness.]]></summary></entry></feed>